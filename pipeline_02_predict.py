# # ğŸ”® Daily Prediction & Fine-tuning Pipeline - v7E highAccuracy dynH
# **Version 7E - 5-Fold Ensemble + 3-Level Stacking + Regime Dynamic**
# **ëª¨ë¸ ê²½ë¡œ: `2526Winter_Sideproject/models/production/v7E_production_highAccuracy_dynH`**
# 
# ## ğŸ¯ ì£¼ìš” ê¸°ëŠ¥:
# 1. ğŸ“Š Supabase ë°ì´í„° ë¡œë“œ & ê²€ì¦
# 2. ğŸ” **ê³¼ê±° ì˜ˆì¸¡ ê²€ì¦** (01a í†µí•©: is_correct NULL â†’ 24h ë’¤ Binance/Upbit 1ë¶„ìº”ë“¤ë¡œ ì •í™• ê²€ì¦)
# 3. ğŸ¤– ëª¨ë¸ ë¡œë“œ (CatBoost, 5x CNN-LSTM, 5x PatchTST)
# 4. ğŸ§  **3-Level Stacking Meta-Learner ì•™ìƒë¸”**
# 5. ğŸ“š ì¦ë¶„ í•™ìŠµ (Fine-tuning)
# 6. ğŸ¯ **Regime-Based Dynamic Ensemble**
# 7. ğŸ”® ë‚´ì¼ ê°€ê²© ì˜ˆì¸¡ (UP/DOWN)
# 8. ğŸ’¾ ì˜ˆì¸¡ ê²°ê³¼ Supabase ì €ì¥
# 9. â™»ï¸ í•™ìŠµëœ ëª¨ë¸ ë®ì–´ì“°ê¸°
# 
# ## ğŸ“‹ Kaggle í•™ìŠµ Output íŒŒì¼:
# - `cnnlstm_f0.pth` ~ `cnnlstm_f4.pth` (5-Fold CNN-LSTM)
# - `patchtst_f0.pth` ~ `patchtst_f4.pth` (5-Fold PatchTST)
# - `scalers.pkl` (RobustScaler Ã— 3)
# - `model_features.json` (Feature lists Ã— 3)
# - `meta_models.pkl` (XGBoost L2 + L3)
# - `confidence_accuracy_coeffs.json`

# ## ğŸ“¦ 0. íŒ¨í‚¤ì§€ ì„¤ì¹˜ & ì„í¬íŠ¸

# í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜

# ==========================================
# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
# ==========================================
import os
import sys
import json
import pickle
import logging
import warnings
import gc
import math
import time
from datetime import datetime, timezone, timedelta

import numpy as np
import pandas as pd
import joblib

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset, Dataset

from sklearn.preprocessing import RobustScaler
from sklearn.metrics import accuracy_score, f1_score

from catboost import CatBoostClassifier
from xgboost import XGBClassifier

import requests

warnings.filterwarnings('ignore')

print('âœ… ëª¨ë“  íŒ¨í‚¤ì§€ë¥¼ ì„±ê³µì ìœ¼ë¡œ ì„í¬íŠ¸í–ˆìŠµë‹ˆë‹¤!')

# ==========================================
# ë¡œê¹… ì„¤ì • (KST ì‹œê°„) + Colab ì¶œë ¥ ë³´ì¥
# ==========================================
KST = timezone(timedelta(hours=9))

def log(message, important=False):
    """
    ì»¤ìŠ¤í…€ ë¡œê·¸ í•¨ìˆ˜
    - í•­ìƒ KST íƒ€ì„ìŠ¤íƒ¬í”„ì™€ í•¨ê»˜ print (Colab ì¶œë ¥ ë³´ì¥)
    - important=True ë©´ ëˆˆì— ë„ëŠ” êµ¬ë¶„ì„  ì¶”ê°€
    """
    kst_now = datetime.now(KST).strftime('%Y-%m-%d %H:%M:%S')
    msg = str(message)
    if important:
        print(f'\n{"*"*60}')
        print(f'[{kst_now}] â­ {msg}')
        print(f'{"*"*60}')
    else:
        print(f'[{kst_now}] {msg}')
    sys.stdout.flush()  # Colab ë²„í¼ ì¦‰ì‹œ í”ŒëŸ¬ì‹œ

log('âœ… ë¡œê¹… ì‹œìŠ¤í…œì´ KST(í•œêµ­ ì‹œê°„) ê¸°ì¤€ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.')

# ============================================================
# ğŸ’° ì‹¤ì‹œê°„ BTC ê°€ê²© ì¡°íšŒ í•¨ìˆ˜ (USD + KRW)
# ============================================================

def get_realtime_btc_price_usd():
    """ì‹¤ì‹œê°„ BTC USD ê°€ê²© ì¡°íšŒ (CoinGecko -> Binance fallback)"""
    try:
        r = requests.get('https://api.coingecko.com/api/v3/simple/price?ids=bitcoin&vs_currencies=usd', timeout=5)
        if r.status_code == 200:
            return float(r.json()['bitcoin']['usd'])
    except:
        pass
    try:
        r = requests.get('https://api.binance.com/api/v3/ticker/price?symbol=BTCUSDT', timeout=5)
        if r.status_code == 200:
            return float(r.json()['price'])
    except:
        pass
    return None

def get_krw_bitcoin_price():
    """ì‹¤ì‹œê°„ BTC KRW ê°€ê²© ì¡°íšŒ (Upbit)"""
    try:
        r = requests.get('https://api.upbit.com/v1/ticker?markets=KRW-BTC', timeout=5)
        if r.status_code == 200:
            return float(r.json()[0]['trade_price'])
    except:
        pass
    return None

# í˜„ì¬ ì‹œì„¸ í‘œì‹œ
usd_price = get_realtime_btc_price_usd()
krw_price = get_krw_bitcoin_price()
now_kst = datetime.now(KST)
target_kst = now_kst + timedelta(hours=24)

print(f'\n{"="*60}')
print(f'ğŸ’° ì‹¤ì‹œê°„ ë¹„íŠ¸ì½”ì¸ ì‹œì„¸')
print(f'{"="*60}')
if usd_price:
    print(f'   ğŸ‡ºğŸ‡¸ USD: ${usd_price:,.2f}')
if krw_price:
    print(f'   ğŸ‡°ğŸ‡· KRW: â‚©{krw_price:,.0f}')
if usd_price and krw_price:
    implicit_rate = krw_price / usd_price
    print(f'   í™˜ìœ¨(ì•”ë¬µì ): {implicit_rate:,.1f} KRW/USD')
print(f'\n   â° ì˜ˆì¸¡: {now_kst.strftime("%Y-%m-%d %H:%M")} â†’ {target_kst.strftime("%Y-%m-%d %H:%M")}')

log('âœ… ì‹¤ì‹œê°„ ê°€ê²© ì¡°íšŒ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ')

# ## ğŸ› ï¸ 1. í™˜ê²½ ì„¤ì • & Supabase ì´ˆê¸°í™”

# ==============================================================================
# ğŸ” 1. ì–´ë–¤ í™˜ê²½ì—ì„œë“  ì•Œì•„ì„œ í‚¤ë¥¼ ì°¾ì•„ì˜¤ëŠ” í•˜ì´ë¸Œë¦¬ë“œ ë¡œë“œ êµ¬ì„±
# ==============================================================================
import sys
import os

IS_COLAB = 'google.colab' in sys.modules
IS_KAGGLE = 'kaggle_secrets' in sys.modules or os.path.exists('/kaggle/working')
IS_GITHUB_ACTIONS = os.getenv('GITHUB_ACTIONS') == 'true'

# [ë¡œì»¬ & ê¸°ì¡´ Colab ë“œë¼ì´ë¸Œ ì‚¬ìš©ììš©] .env íŒŒì¼ ë¡œë“œ
try:
    from dotenv import load_dotenv
    for env_path in ['/content/drive/MyDrive/2526Winter_Sideproject/.env', '.env', '/content/.env']:
        if os.path.exists(env_path):
            load_dotenv(env_path)
            log(f"ğŸ”§ .env íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {env_path}")
            break
except ImportError:
    pass

# [Colab Secrets / Kaggle Secrets ì „ìš© ì‚¬ìš©ììš©]
if IS_COLAB:
    from google.colab import drive, userdata
    drive.mount('/content/drive')
    os.environ['PROJECT_ROOT'] = '/content/drive/MyDrive/2526Winter_Sideproject'
    for key in ['SUPABASE_URL', 'SUPABASE_KEY', 'SUPABASE_SERVICE_KEY', 'OPENAI_API_KEY']:
        try:
            val = userdata.get(key)
            if val: os.environ[key] = val
        except: pass
elif IS_KAGGLE:
    from kaggle_secrets import UserSecretsClient
    user_secrets = UserSecretsClient()
    os.environ['PROJECT_ROOT'] = '/kaggle/working'
    for key in ['SUPABASE_URL', 'SUPABASE_KEY', 'SUPABASE_SERVICE_KEY', 'OPENAI_API_KEY']:
        try:
            val = user_secrets.get_secret(key)
            if val: os.environ[key] = val
        except: pass
elif IS_GITHUB_ACTIONS:
    os.environ['PROJECT_ROOT'] = os.getenv('GITHUB_WORKSPACE', os.getcwd())
else:
    os.environ['PROJECT_ROOT'] = os.getcwd()

# ==============================================================================
# ğŸš€ 2. ë³€ìˆ˜ í• ë‹¹ ë° ëª¨ë¸/Supabase ì—°ê²°
# ==============================================================================
PROJECT_ROOT = os.getenv("PROJECT_ROOT", os.getcwd())
MODEL_DIR = os.path.join(PROJECT_ROOT, 'models', 'production', 'v7E_production_highAccuracy_dynH')

log(f"ğŸ’» í™˜ê²½ ì…‹ì—… ì™„ë£Œ")
log(f'  PROJECT_ROOT: {PROJECT_ROOT}')
log(f'  MODEL_DIR: {MODEL_DIR}')
os.makedirs(MODEL_DIR, exist_ok=True)

SUPABASE_URL = os.getenv("SUPABASE_URL")
SERVICE_KEY = os.getenv("SUPABASE_SERVICE_KEY", os.getenv("SUPABASE_KEY")) 

if not SUPABASE_URL or not SERVICE_KEY:
    raise ValueError("âŒ ì¹˜ëª…ì  ì˜¤ë¥˜: SUPABASE_URL ë˜ëŠ” SUPABASE_SERVICE_KEYë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")

from supabase import create_client
supabase = create_client(SUPABASE_URL, SERVICE_KEY)

log("âœ… Supabase ë§ˆìŠ¤í„° ê¶Œí•œ(Service Role) ì—°ê²° ì™„ë£Œ ğŸ”“")

# ==============================================================================
# ğŸ“¥ [GitHub Actions ì „ìš©] Supabase Storageì—ì„œ ëª¨ë¸ ê°€ì¤‘ì¹˜ ìë™ ë‹¤ìš´ë¡œë“œ
# ==============================================================================
if IS_GITHUB_ACTIONS:
    log("ğŸ”½ GitHub Actions: Supabase Storageì—ì„œ ëª¨ë¸ ê°€ì¤‘ì¹˜ ë‹¤ìš´ë¡œë“œ ì‹œì‘...", important=True)
    STORAGE_BUCKET = "models"
    STORAGE_FOLDER = "daily_v7e_dynH"
    FILES_TO_DOWNLOAD = [
        "cnnlstm_f0.pth", "cnnlstm_f1.pth", "cnnlstm_f2.pth", "cnnlstm_f3.pth", "cnnlstm_f4.pth",
        "patchtst_f0.pth", "patchtst_f1.pth", "patchtst_f2.pth", "patchtst_f3.pth", "patchtst_f4.pth",
        "scalers.pkl", "meta_models.pkl", "model_features.json", "confidence_accuracy_coeffs.json",
    ]
    for filename in FILES_TO_DOWNLOAD:
        dest_path = os.path.join(MODEL_DIR, filename)
        if os.path.exists(dest_path):
            log(f"  âœ… ì´ë¯¸ ì¡´ì¬: {filename}")
            continue
        try:
            storage_path = f"{STORAGE_FOLDER}/{filename}"
            data = supabase.storage.from_(STORAGE_BUCKET).download(storage_path)
            with open(dest_path, "wb") as f:
                f.write(data)
            log(f"  âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {filename} ({len(data):,} bytes)")
        except Exception as e:
            log(f"  âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {filename} â†’ {e}")
            raise
    log("ğŸ‰ ëª¨ë“  ëª¨ë¸ ê°€ì¤‘ì¹˜ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!", important=True)


# ## âš™ï¸ 2. Config & ëª¨ë¸ ì•„í‚¤í…ì²˜ (v7E)

# ==========================================
# Config - dynH í•™ìŠµê³¼ ë™ì¼ (monster_kaggle_v7E_dynH)
# ==========================================
class Config:
    """dynH í•™ìŠµ ì„¤ì •ê³¼ ì™„ë²½íˆ ë™ì¼í•œ íŒŒë¼ë¯¸í„°"""

    # ========== Window ì„¤ì • ==========
    SEQUENCE_LENGTHS = [24, 48, 72]
    PRIMARY_SEQUENCE_LENGTH = 72
    PREDICTION_HORIZON = 24

    # ========== CNN-LSTM (dynH: 3ì¸µ 256â†’128, LSTM 3â†’2) ==========
    CNN_KERNEL_SIZES = [3, 5, 7]
    CNN_FILTERS = [64, 128, 128]  # [dynH] 3ì¸µ ë‹¨ìˆœí™”
    LSTM_HIDDEN = 256
    LSTM_LAYERS = 2  # [dynH] 3â†’2
    LSTM_BIDIRECTIONAL = True

    # ========== Transformer / PatchTST (dynH: PATCH_LEN 48, STRIDE 24) ==========
    N_HEADS = 8
    D_MODEL = 256
    N_LAYERS = 4
    PATCH_LEN = 48  # [dynH] 24â†’48h: BTC 2ì¼ ì‚¬ì´í´
    STRIDE = 24     # [dynH] PATCH_LEN/2
    DROPOUT = 0.4   # [dynH] ì •ê·œí™” ê°•í™”

    # ========== í•™ìŠµ íŒŒë¼ë¯¸í„° ==========
    BATCH_SIZE = 32
    EPOCHS = 100
    LEARNING_RATE = 3e-4
    MIN_LR = 1e-6
    WARMUP_EPOCHS = 5
    PATIENCE = 20
    ACCUM_STEPS = 4
    FOCAL_GAMMA = 2.0
    FOCAL_ALPHA = 0.25
    LABEL_SMOOTHING = 0.1

    # ========== v7E ì „ìš© ==========
    USE_REVIN = True
    USE_MIXUP = True
    MIXUP_ALPHA = 0.2
    USE_SWA = True
    SWA_START = 60

    # ========== CatBoost (on-the-fly í•™ìŠµìš©) ==========
    CATBOOST_ITERATIONS = 3000
    CATBOOST_DEPTH = 6
    CATBOOST_LR = 0.03
    CATBOOST_L2 = 5
    CATBOOST_EARLY_STOPPING = 200

    # ========== Dynamic Threshold ==========
    REGIME_WINDOW = 168  # 7ì¼ (168ì‹œê°„)

    # ========== 5-Fold ==========
    N_FOLDS = 5

    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

config = Config()
log(f'Config: HORIZON={config.PREDICTION_HORIZON}h, SEQ_LEN={config.PRIMARY_SEQUENCE_LENGTH}, FOLDS={config.N_FOLDS}, DEVICE={config.DEVICE}')

# ==========================================
# v7E ëª¨ë¸ ì•„í‚¤í…ì²˜ (Kaggle í•™ìŠµê³¼ ë™ì¼)
# ==========================================

class RevIN(nn.Module):
    """Reversible Instance Normalization"""
    def __init__(self, n_features, eps=1e-5, affine=True):
        super().__init__()
        self.eps, self.affine = eps, affine
        if affine:
            self.gamma = nn.Parameter(torch.ones(n_features))
            self.beta = nn.Parameter(torch.zeros(n_features))
    def forward(self, x, mode='norm'):
        if mode == 'norm':
            self.mean = x.mean(dim=1, keepdim=True)
            self.std = x.std(dim=1, keepdim=True) + self.eps
            x = (x - self.mean) / self.std
            if self.affine: x = x * self.gamma + self.beta
        elif mode == 'denorm':
            if self.affine: x = (x - self.beta) / self.gamma
            x = x * self.std + self.mean
        return x

class FocalLoss(nn.Module):
    """Focal Loss for class imbalance"""
    def __init__(self, gamma=2.0, alpha=0.25, smoothing=0.1):
        super().__init__()
        self.gamma, self.alpha, self.smoothing = gamma, alpha, smoothing
    def forward(self, inputs, targets):
        ce = F.cross_entropy(inputs, targets, reduction='none')
        pt = torch.exp(-ce)
        return (self.alpha * (1 - pt) ** self.gamma * ce).mean()

class SEBlock(nn.Module):
    """Squeeze-and-Excitation Block"""
    def __init__(self, ch, ratio=4):
        super().__init__()
        self.fc = nn.Sequential(
            nn.AdaptiveAvgPool1d(1), nn.Flatten(),
            nn.Linear(ch, ch // ratio), nn.ReLU(),
            nn.Linear(ch // ratio, ch), nn.Sigmoid()
        )
    def forward(self, x):
        return x * self.fc(x).unsqueeze(-1)

class MultiScaleCNN(nn.Module):
    """Multi-scale CNN with SE Block (v7E)"""
    def __init__(self, in_ch, out_ch):
        super().__init__()
        self.c3 = nn.Conv1d(in_ch, out_ch, 3, padding=1)
        self.c5 = nn.Conv1d(in_ch, out_ch, 5, padding=2)
        self.c7 = nn.Conv1d(in_ch, out_ch, 7, padding=3)
        self.bn = nn.BatchNorm1d(out_ch * 3)
        self.se = SEBlock(out_ch * 3)
        self.drop = nn.Dropout(config.DROPOUT)
    def forward(self, x):
        out = torch.cat([self.c3(x), self.c5(x), self.c7(x)], dim=1)
        return self.drop(self.se(F.gelu(self.bn(out))))

class EnhancedCNNLSTM(nn.Module):
    """Enhanced CNN-LSTM (v7E - with RevIN, SEBlock, GELU)"""
    def __init__(self, n_feat, seq_len):
        super().__init__()
        self.revin = RevIN(n_feat) if config.USE_REVIN else None
        self.cnn1 = MultiScaleCNN(n_feat, config.CNN_FILTERS[0])
        self.cnn2 = MultiScaleCNN(config.CNN_FILTERS[0] * 3, config.CNN_FILTERS[1])
        self.cnn3 = MultiScaleCNN(config.CNN_FILTERS[1] * 3, config.CNN_FILTERS[2])
        self.pool = nn.MaxPool1d(2)
        self.lstm = nn.LSTM(
            config.CNN_FILTERS[2] * 3, config.LSTM_HIDDEN, config.LSTM_LAYERS,
            batch_first=True, bidirectional=True, dropout=config.DROPOUT
        )
        self.attn = nn.MultiheadAttention(
            config.LSTM_HIDDEN * 2, config.N_HEADS,
            dropout=config.DROPOUT, batch_first=True
        )
        self.classifier = nn.Sequential(
            nn.LayerNorm(config.LSTM_HIDDEN * 2),
            nn.Linear(config.LSTM_HIDDEN * 2, config.LSTM_HIDDEN),
            nn.GELU(), nn.Dropout(config.DROPOUT),
            nn.Linear(config.LSTM_HIDDEN, config.LSTM_HIDDEN // 2),
            nn.GELU(), nn.Dropout(config.DROPOUT),
            nn.Linear(config.LSTM_HIDDEN // 2, 2)
        )

    def forward(self, x):
        if self.revin:
            x = self.revin(x, 'norm')
        x = x.permute(0, 2, 1)
        x = self.pool(self.cnn1(x))
        x = self.pool(self.cnn2(x))
        x = self.pool(self.cnn3(x))
        x = x.permute(0, 2, 1)
        out, _ = self.lstm(x)
        out, _ = self.attn(out, out, out)
        return self.classifier(out.mean(dim=1) + out[:, -1, :])

class EnhancedPatchTST(nn.Module):
    """Enhanced PatchTST (v7E - with RevIN, mean+max pooling)"""
    def __init__(self, n_feat, seq_len):
        super().__init__()
        self.revin = RevIN(n_feat) if config.USE_REVIN else None
        self.seq_len = seq_len
        self.n_patches = (seq_len - config.PATCH_LEN) // config.STRIDE + 1
        self.embed = nn.Sequential(
            nn.Linear(config.PATCH_LEN * n_feat, config.D_MODEL),
            nn.LayerNorm(config.D_MODEL), nn.GELU(), nn.Dropout(config.DROPOUT)
        )
        self.pos = nn.Parameter(torch.randn(1, self.n_patches, config.D_MODEL))
        enc = nn.TransformerEncoderLayer(
            config.D_MODEL, config.N_HEADS, config.D_MODEL * 4,
            config.DROPOUT, activation='gelu', batch_first=True, norm_first=True
        )
        self.trans = nn.TransformerEncoder(enc, config.N_LAYERS)
        self.classifier = nn.Sequential(
            nn.LayerNorm(config.D_MODEL),
            nn.Linear(config.D_MODEL, config.D_MODEL // 2),
            nn.GELU(), nn.Dropout(config.DROPOUT),
            nn.Linear(config.D_MODEL // 2, config.D_MODEL // 4),
            nn.GELU(), nn.Dropout(config.DROPOUT),
            nn.Linear(config.D_MODEL // 4, 2)
        )

    def forward(self, x):
        if self.revin:
            x = self.revin(x, 'norm')
        B = x.shape[0]
        patches = [
            x[:, i * config.STRIDE : i * config.STRIDE + config.PATCH_LEN, :].reshape(B, -1)
            for i in range(self.n_patches)
        ]
        x = self.embed(torch.stack(patches, dim=1)) + self.pos
        x = self.trans(x)
        return self.classifier(x.mean(dim=1) + x.max(dim=1)[0])

log('âœ… v7E ëª¨ë¸ ì•„í‚¤í…ì²˜ ì •ì˜ ì™„ë£Œ (RevIN + SEBlock + MultiScaleCNN)')

# ## ğŸ“Š 3. ë°ì´í„° ë¡œë“œ & ì „ì²˜ë¦¬

# ==========================================
# Supabase ë°ì´í„° ë¡œë“œ í•¨ìˆ˜
# ==========================================

def fetch_all_features_master():
    """features_master í…Œì´ë¸”ì—ì„œ ì „ì²´ ë°ì´í„° ë¡œë“œ (íƒ€ì„ì•„ì›ƒ ë°©ì§€)"""
    log('ğŸ“Š Supabase features_master ë°ì´í„° ë¡œë“œ ì¤‘...')
    all_rows, offset = [], 0
    batch_size = 500  # íƒ€ì„ì•„ì›ƒ ë°©ì§€ë¥¼ ìœ„í•´ ë°°ì¹˜ í¬ê¸° ì¶•ì†Œ
    
    while True:
        try:
            # íƒ€ì„ì•„ì›ƒ ë°©ì§€: ì‘ì€ ë°°ì¹˜ + ì¬ì‹œë„
            result = supabase.table('features_master').select('*').order('date').range(offset, offset + batch_size - 1).execute()
            if not result.data:
                break
            all_rows.extend(result.data)
            offset += len(result.data)
            
            # ì§„í–‰ìƒí™© í‘œì‹œ (ë§¤ 5000ê°œë§ˆë‹¤)
            if offset % 5000 == 0:
                log(f'  ì§„í–‰: {offset:,} rows ë¡œë“œë¨...')
            
            if len(result.data) < batch_size:
                break
                
        except Exception as e:
            if 'timeout' in str(e).lower():
                log(f'  âš ï¸ íƒ€ì„ì•„ì›ƒ ë°œìƒ (offset={offset}), ë°°ì¹˜ í¬ê¸° ì¶•ì†Œ í›„ ì¬ì‹œë„...')
                batch_size = max(100, batch_size // 2)  # ë°°ì¹˜ í¬ê¸° ì ˆë°˜ìœ¼ë¡œ
                continue
            else:
                raise
    
    df = pd.DataFrame(all_rows)
    df['date'] = pd.to_datetime(df['date'])
    df = df.sort_values('date').reset_index(drop=True)
    log(f'  âœ… features_master: {len(df):,} rows ({df["date"].min()} ~ {df["date"].max()})')
    return df

def fetch_sentiment_data():
    """raw_sentiment í…Œì´ë¸”ì—ì„œ ê°ì„± ë°ì´í„° ë¡œë“œ"""
    log('ğŸ“Š raw_sentiment ë°ì´í„° ë¡œë“œ ì¤‘...')
    all_rows, offset = [], 0
    while True:
        result = supabase.table('raw_sentiment').select('date,sentiment_score,impact_score').order('date').range(offset, offset + 999).execute()
        if not result.data:
            break
        all_rows.extend(result.data)
        if len(result.data) < 1000:
            break
        offset += 1000
    if not all_rows:
        return pd.DataFrame()
    df = pd.DataFrame(all_rows)
    df['date'] = pd.to_datetime(df['date'])
    log(f'  raw_sentiment: {len(df):,} rows')
    return df

def load_and_prepare_data(recent_days=None):
    """
    ë°ì´í„° ë¡œë“œ + ê°ì„± ë³‘í•© + target ìƒì„±
    v7E í•™ìŠµê³¼ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬
    
    Args:
        recent_days: Noneì´ë©´ ì „ì²´, ìˆ«ìë©´ ìµœê·¼ Nì¼ë§Œ ë¡œë“œ (íƒ€ì„ì•„ì›ƒ ë°©ì§€)
    """
    if recent_days is not None:
        log(f'ğŸ“Š ìµœê·¼ {recent_days}ì¼ ë°ì´í„°ë§Œ ë¡œë“œ (íƒ€ì„ì•„ì›ƒ ë°©ì§€)')
        cutoff_date = (datetime.now(timezone.utc) - timedelta(days=recent_days)).strftime('%Y-%m-%d')
        log(f'  ê¸°ì¤€ì¼: {cutoff_date}')
        
        all_rows, offset = [], 0
        while True:
            result = supabase.table('features_master').select('*').gte('date', cutoff_date).order('date').range(offset, offset + 999).execute()
            if not result.data:
                break
            all_rows.extend(result.data)
            offset += len(result.data)
            if len(result.data) < 1000:
                break
        df = pd.DataFrame(all_rows)
        df['date'] = pd.to_datetime(df['date'])
        log(f'  âœ… features_master: {len(df):,} rows ({df["date"].min()} ~ {df["date"].max()})')
    else:
        df = fetch_all_features_master()
    df_sent = fetch_sentiment_data()

    # v7E í•™ìŠµê³¼ ë™ì¼: features_masterì˜ sentiment ì œê±° í›„ raw_sentimentì—ì„œ ë³‘í•©
    if not df_sent.empty:
        for col in ['sentiment_score', 'impact_score']:
            if col in df.columns:
                df = df.drop(columns=[col], errors='ignore')
        df = pd.merge(df, df_sent, on='date', how='left')

    # ê°ì„± ê²°ì¸¡ì¹˜ ì±„ìš°ê¸°
    if 'sentiment_score' not in df.columns:
        df['sentiment_score'] = 0
    if 'impact_score' not in df.columns:
        df['impact_score'] = 0.5
    df['sentiment_score'] = df['sentiment_score'].fillna(0)
    df['impact_score'] = df['impact_score'].fillna(0.5)

    # close ì»¬ëŸ¼ ì´ë¦„ í†µì¼
    if 'close' not in df.columns and 'close_price' in df.columns:
        df['close'] = df['close_price']
    elif 'close' not in df.columns:
        close_candidates = [c for c in df.columns if 'close' in c.lower()]
        if close_candidates:
            df['close'] = pd.to_numeric(df[close_candidates[0]], errors='coerce')
            log(f'  âš ï¸ close ì»¬ëŸ¼ìœ¼ë¡œ "{close_candidates[0]}" ì‚¬ìš©')

    # target ìƒì„±: 24ì‹œê°„ ë’¤ ê°€ê²© ìƒìŠ¹ ì—¬ë¶€
    df['close'] = pd.to_numeric(df['close'], errors='coerce')
    df['target'] = (df['close'].shift(-config.PREDICTION_HORIZON) > df['close']).astype(int)

    df = df.sort_values('date').reset_index(drop=True)
    log(f'âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {len(df):,} rows, target NaN(ë§ˆì§€ë§‰ 24h): {df["target"].isna().sum()}')
    return df


# ==========================================
# On-the-fly íŒŒìƒ í”¼ì²˜ ìƒì„± (Kaggle v7E í•™ìŠµê³¼ ë™ì¼)
# ==========================================

def add_on_the_fly_features(df):
    """
    Kaggle v7E í•™ìŠµ ì‹œ on-the-flyë¡œ ìƒì„±ëœ íŒŒìƒ í”¼ì²˜ 11ê°œ ì¶”ê°€
    - Cross-Asset Lags: NASDAQ/GOLD/DXY/VIXì˜ lag ë° return
    - Momentum: close ê¸°ë°˜ 4ì‹œê°„ ëª¨ë©˜í…€
    - Market Regime: 7ì¼ ë¶ˆë§ˆì¼“ ì§€í‘œ
    """
    df = df.copy()
    added = []

    # 1. Cross-Asset Lags & Returns
    asset_lag_map = {
        'NASDAQ': [12],       # NASDAQ_lag_12h
        'GOLD': [24],         # GOLD_lag_24h
        'DXY': [12],          # DXY_lag_12h
        'VIX': [12, 24],      # VIX_lag_12h, VIX_lag_24h
    }
    asset_ret_map = {
        'NASDAQ': [24],       # NASDAQ_ret_24h
        'GOLD': [24],         # GOLD_ret_24h
        'DXY': [24],          # DXY_ret_24h
        'VIX': [24],          # VIX_ret_24h
    }

    for asset, lags in asset_lag_map.items():
        if asset in df.columns:
            for lag in lags:
                feat_name = f'{asset}_lag_{lag}h'
                if feat_name not in df.columns:
                    df[feat_name] = df[asset].shift(lag)
                    added.append(feat_name)
        else:
            for lag in lags:
                feat_name = f'{asset}_lag_{lag}h'
                if feat_name not in df.columns:
                    df[feat_name] = 0
                    log(f'  âš ï¸ {asset} ì»¬ëŸ¼ ì—†ìŒ â†’ {feat_name}=0')

    for asset, rets in asset_ret_map.items():
        if asset in df.columns:
            for ret in rets:
                feat_name = f'{asset}_ret_{ret}h'
                if feat_name not in df.columns:
                    df[feat_name] = df[asset].pct_change(ret)
                    added.append(feat_name)
        else:
            for ret in rets:
                feat_name = f'{asset}_ret_{ret}h'
                if feat_name not in df.columns:
                    df[feat_name] = 0
                    log(f'  âš ï¸ {asset} ì»¬ëŸ¼ ì—†ìŒ â†’ {feat_name}=0')

    # 2. Momentum (4ì‹œê°„)
    if 'close' in df.columns and 'momentum_4h' not in df.columns:
        df['momentum_4h'] = df['close'].pct_change(4)
        added.append('momentum_4h')

    # 3. Market Regime (7ì¼ ë¶ˆë§ˆì¼“)
    if 'close' in df.columns and 'regime_bull_7d' not in df.columns:
        df['regime_bull_7d'] = (df['close'] > df['close'].rolling(168, min_periods=1).mean()).astype(int)
        added.append('regime_bull_7d')

    # NaN ì²˜ë¦¬ (lag/rolling ì´ˆê¸°ê°’)
    for col in added:
        df[col] = df[col].bfill().fillna(0)

    log(f'  âœ… On-the-fly íŒŒìƒ í”¼ì²˜ {len(added)}ê°œ ìƒì„±: {added}')
    return df
def get_latest_date_from_supabase():
    """Supabaseì—ì„œ ê°€ì¥ ìµœê·¼ ë‚ ì§œ ì¡°íšŒ"""
    try:
        result = supabase.table('features_master').select('date').order('date', desc=True).limit(1).execute()
        if result.data:
            return pd.to_datetime(result.data[0]['date'])
    except:
        pass
    return None

def get_model_metadata():
    """ëª¨ë¸ ë©”íƒ€ë°ì´í„° ë¡œë“œ (ë§ˆì§€ë§‰ í•™ìŠµ ë‚ ì§œ ë“±)"""
    metadata_path = os.path.join(MODEL_DIR, 'model_metadata.json')
    if os.path.exists(metadata_path):
        with open(metadata_path, 'r') as f:
            return json.load(f)
    return None

log('âœ… ë°ì´í„° ë¡œë“œ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ')

# ## ğŸ¤– 4. ëª¨ë¸ ë¡œë“œ (5-Fold + Meta-Learner + CatBoost)

# ==========================================
# ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜
# ==========================================

def load_model_features_v7e():
    """
    model_features.json ë¡œë“œ
    v7E í˜•ì‹: {'catboost': [...], 'cnnlstm': [...], 'patchtst': [...]}
    ê¸°ì¡´ í˜•ì‹: {'Boruta_CatBoost_121': [...], ...}
    """
    features_path = os.path.join(MODEL_DIR, 'model_features.json')
    if not os.path.exists(features_path):
        features_path = os.path.join(PROJECT_ROOT, 'model_features.json')
    if not os.path.exists(features_path):
        raise FileNotFoundError(f'model_features.jsonì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤')

    with open(features_path, 'r') as f:
        data = json.load(f)

    # v7E í˜•ì‹ ìš°ì„ 
    if 'catboost' in data:
        features = {
            'catboost': [f for f in data['catboost'] if f != 'date'],
            'cnnlstm': [f for f in data['cnnlstm'] if f != 'date'],
            'patchtst': [f for f in data['patchtst'] if f != 'date'],
        }
    else:
        # ê¸°ì¡´ Boruta í˜•ì‹ í˜¸í™˜
        cat_key = [k for k in data.keys() if 'CatBoost' in k or 'catboost' in k][0]
        cnn_key = [k for k in data.keys() if 'CNNLSTM' in k or 'cnnlstm' in k][0]
        patch_key = [k for k in data.keys() if 'PatchTST' in k or 'patchtst' in k][0]
        features = {
            'catboost': [f for f in data[cat_key] if f != 'date'],
            'cnnlstm': [f for f in data[cnn_key] if f != 'date'],
            'patchtst': [f for f in data[patch_key] if f != 'date'],
        }

    log(f'  í”¼ì²˜ ë¡œë“œ: CatBoost={len(features["catboost"])}, CNN-LSTM={len(features["cnnlstm"])}, PatchTST={len(features["patchtst"])}')
    return features

def load_scalers_v7e():
    """scalers.pkl ë¡œë“œ (v7E: {'catboost': sc, 'cnnlstm': sc, 'patchtst': sc})"""
    scalers_path = os.path.join(MODEL_DIR, 'scalers.pkl')
    if not os.path.exists(scalers_path):
        raise FileNotFoundError(f'scalers.pklì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {scalers_path}')
    with open(scalers_path, 'rb') as f:
        scalers = pickle.load(f)
    log(f'  ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ì™„ë£Œ')
    return scalers

def load_fold_models(model_type, n_features, seq_len):
    """5-Fold ëª¨ë¸ ë¡œë“œ (cnnlstm or patchtst)"""
    models = []
    for fold in range(config.N_FOLDS):
        filename = f'{model_type}_f{fold}.pth'
        filepath = os.path.join(MODEL_DIR, filename)
        if not os.path.exists(filepath):
            log(f'  âš ï¸ {filename} ì—†ìŒ - ê±´ë„ˆëœ€')
            continue

        checkpoint = torch.load(filepath, map_location=config.DEVICE, weights_only=False)

        if model_type == 'cnnlstm':
            model = EnhancedCNNLSTM(n_features, seq_len)
        elif model_type == 'patchtst':
            model = EnhancedPatchTST(n_features, seq_len)
        else:
            raise ValueError(f'Unknown model_type: {model_type}')

        # v7E checkpoint í˜•ì‹: {'model_state_dict': ...}
        if 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'], strict=False)
        elif 'model_state' in checkpoint:
            model.load_state_dict(checkpoint['model_state'], strict=False)
        else:
            model.load_state_dict(checkpoint, strict=False)

        model = model.to(config.DEVICE)
        model.eval()
        models.append(model)

    log(f'  {model_type}: {len(models)}/{config.N_FOLDS} ëª¨ë¸ ë¡œë“œ ì™„ë£Œ')
    return models

def load_meta_models_v7e():
    """meta_models.pkl ë¡œë“œ: dynG í˜•ì‹ {'blend_weights': [...], 'best_strategy': '...'}"""
    meta_path = os.path.join(MODEL_DIR, 'meta_models.pkl')
    if not os.path.exists(meta_path):
        log('  âš ï¸ meta_models.pkl ì—†ìŒ - ë‹¨ìˆœ í‰ê·  í´ë°± ì‚¬ìš©')
        return None, 'A: Simple Avg'
    with open(meta_path, 'rb') as f:
        meta = pickle.load(f)
    blend_weights = meta.get('blend_weights')
    best_strategy = meta.get('best_strategy', 'A: Simple Avg')
    log(f'  âœ… dynG Meta ë¡œë“œ: best_strategy={best_strategy}')
    if blend_weights:
        log(f'  OOF Blend Weights: Cat={blend_weights[0]:.3f}, CNN={blend_weights[1]:.3f}, Patch={blend_weights[2]:.3f}')
    return blend_weights, best_strategy

def load_confidence_coeffs():
    """confidence_accuracy_coeffs.json ë¡œë“œ"""
    coeff_path = os.path.join(MODEL_DIR, 'confidence_accuracy_coeffs.json')
    if not os.path.exists(coeff_path):
        log('  âš ï¸ confidence_accuracy_coeffs.json ì—†ìŒ - ê¸°ë³¸ê°’ ì‚¬ìš©')
        return {'a': 0, 'b': 1, 'c': 0}
    with open(coeff_path, 'r') as f:
        coeffs = json.load(f)
    log(f'  âœ… Confidence-Accuracy ê³„ìˆ˜: a={coeffs["a"]:.6f}, b={coeffs["b"]:.6f}, c={coeffs["c"]:.4f}')
    return coeffs

def train_catboost_from_data(df, features, scaler):
    """
    CatBoost ëª¨ë¸ on-the-fly í•™ìŠµ
    (Kaggle í•™ìŠµì—ì„œ CatBoost .cbm íŒŒì¼ì´ ì €ì¥ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ,
     Supabase ë°ì´í„°ë¡œ ì§ì ‘ í•™ìŠµ)
    """
    log('ğŸš€ CatBoost on-the-fly í•™ìŠµ ì‹œì‘...', important=True)

    # targetì´ ìˆëŠ” í–‰ë§Œ ì‚¬ìš©
    df_valid = df.dropna(subset=['target']).copy()
    if len(df_valid) < 100:
        log('  âš ï¸ í•™ìŠµ ë°ì´í„° ë¶€ì¡± - CatBoost ìƒëµ')
        return None

    # í”¼ì²˜ ì„ íƒ ë° ìŠ¤ì¼€ì¼ë§ (ëˆ„ë½ í”¼ì²˜ëŠ” 0ìœ¼ë¡œ ì±„ì›€)
    missing_feat = [f for f in features if f not in df_valid.columns]
    if missing_feat:
        log(f"  âš ï¸ CatBoost ëˆ„ë½ í”¼ì²˜ {len(missing_feat)}ê°œ â†’ 0ìœ¼ë¡œ ì±„ì›€")
        for f in missing_feat:
            df_valid[f] = 0

    if len([f for f in features if f in df_valid.columns]) == 0:
        log('  âŒ ìœ íš¨í•œ CatBoost í”¼ì²˜ ì—†ìŒ')
        return None

    X = df_valid[features].fillna(0).values  # ì „ì²´ í”¼ì²˜ ì‚¬ìš© (ìŠ¤ì¼€ì¼ëŸ¬ í˜¸í™˜)
    y = df_valid['target'].values.astype(int)

    # ì €ì¥ëœ ìŠ¤ì¼€ì¼ëŸ¬ë¡œ ë³€í™˜ (ì¼ê´€ì„± ìœ ì§€)
    X_scaled = scaler.transform(X)

    # ì‹œê³„ì—´ ê¸°ë°˜ ë¶„í• : ë§ˆì§€ë§‰ 20%ë¥¼ validationìœ¼ë¡œ
    split_idx = int(len(X_scaled) * 0.8)
    X_tr, X_va = X_scaled[:split_idx], X_scaled[split_idx:]
    y_tr, y_va = y[:split_idx], y[split_idx:]

    # v7E í•™ìŠµê³¼ ë™ì¼í•œ íŒŒë¼ë¯¸í„°
    cat_model = CatBoostClassifier(
        iterations=config.CATBOOST_ITERATIONS,
        depth=config.CATBOOST_DEPTH,
        learning_rate=config.CATBOOST_LR,
        l2_leaf_reg=config.CATBOOST_L2,
        loss_function='Logloss',
        eval_metric='Accuracy',
        early_stopping_rounds=config.CATBOOST_EARLY_STOPPING,
        verbose=0,
        task_type='GPU' if torch.cuda.is_available() else 'CPU',
        random_seed=42
    )
    cat_model.fit(X_tr, y_tr, eval_set=(X_va, y_va), use_best_model=True)

    val_preds = cat_model.predict(X_va)
    val_acc = accuracy_score(y_va, val_preds)
    log(f'  âœ… CatBoost í•™ìŠµ ì™„ë£Œ: Val Acc={val_acc:.4f} (Train={len(X_tr):,}, Val={len(X_va):,})', important=True)

    # ì €ì¥
    save_path = os.path.join(MODEL_DIR, 'catboost_model_v7e.cbm')
    cat_model.save_model(save_path)
    log(f'  ğŸ’¾ CatBoost ëª¨ë¸ ì €ì¥: {save_path}')

    return cat_model

def load_all_models(df_full=None):
    """
    ëª¨ë“  ëª¨ë¸ ë¡œë“œ (5-Fold + Meta-Learner + CatBoost + ìŠ¤ì¼€ì¼ëŸ¬)
    CatBoostëŠ” íŒŒì¼ì´ ì—†ìœ¼ë©´ on-the-fly í•™ìŠµ
    """
    log('', important=False)
    log('ğŸ¤– ëª¨ë¸ ë¡œë“œ ì‹œì‘', important=True)
    models = {}

    # 1. Feature lists
    models['features'] = load_model_features_v7e()

    # 2. Scalers
    scalers = load_scalers_v7e()
    models['scaler_catboost'] = scalers['catboost']
    models['scaler_cnnlstm'] = scalers['cnnlstm']
    models['scaler_patchtst'] = scalers['patchtst']

    # 3. CNN-LSTM (5-Fold)
    n_feat_cnn = len(models['features']['cnnlstm'])
    models['cnnlstm_models'] = load_fold_models('cnnlstm', n_feat_cnn, config.PRIMARY_SEQUENCE_LENGTH)

    # 4. PatchTST (5-Fold)
    n_feat_patch = len(models['features']['patchtst'])
    models['patchtst_models'] = load_fold_models('patchtst', n_feat_patch, config.PRIMARY_SEQUENCE_LENGTH)

    # 5. Meta-Learner (dynG: blend_weights + best_strategy)
    models['blend_weights'], models['best_strategy'] = load_meta_models_v7e()

    # 6. Confidence-Accuracy ê³„ìˆ˜
    models['confidence_coeffs'] = load_confidence_coeffs()

    # 7. CatBoost (Kaggle ì „ì²´ í•™ìŠµ ëª¨ë¸ + ì¦ë¶„ í•™ìŠµ)
    catboost_production_path = os.path.join(MODEL_DIR, 'catboost_production.cbm')
    catboost_v7e_path = os.path.join(MODEL_DIR, 'catboost_model_v7e.cbm')
    
    if os.path.exists(catboost_production_path):
        # Kaggleì—ì„œ ì „ì²´ ë°ì´í„°ë¡œ í•™ìŠµí•œ í”„ë¡œë•ì…˜ ëª¨ë¸ ë¡œë“œ
        cat_model = CatBoostClassifier()
        cat_model.load_model(catboost_production_path)
        models['catboost_model'] = cat_model
        log(f'  âœ… CatBoost í”„ë¡œë•ì…˜ ëª¨ë¸ ë¡œë“œ: {os.path.basename(catboost_production_path)}')
        
        # ì¦ë¶„ í•™ìŠµ ì—¬ë¶€ëŠ” ë‚˜ì¤‘ì— Step 6ì—ì„œ ê²°ì •
        models['catboost_needs_finetuning'] = True
        
    elif os.path.exists(catboost_v7e_path):
        # ê¸°ì¡´ on-the-fly í•™ìŠµ ëª¨ë¸ (fallback)
        cat_model = CatBoostClassifier()
        cat_model.load_model(catboost_v7e_path)
        models['catboost_model'] = cat_model
        log(f'  âœ… CatBoost ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ: {os.path.basename(catboost_v7e_path)}')
        models['catboost_needs_finetuning'] = False
        
    elif df_full is not None and len(df_full.dropna(subset=['target'])) >= 100:
        # ì €ì¥ëœ ëª¨ë¸ì´ ì—†ìœ¼ë©´ on-the-fly í•™ìŠµ
        log('  âš ï¸ Kaggle í”„ë¡œë•ì…˜ ëª¨ë¸ ì—†ìŒ - on-the-fly í•™ìŠµ ì‹œì‘')
        models['catboost_model'] = train_catboost_from_data(
            df_full, models['features']['catboost'], models['scaler_catboost']
        )
        models['catboost_needs_finetuning'] = False
        
    else:
        log('  âš ï¸ CatBoost ì‚¬ìš© ë¶ˆê°€ - ë°ì´í„°/ëª¨ë¸ ì—†ìŒ')
        models['catboost_model'] = None
        models['catboost_needs_finetuning'] = False

    # ìš”ì•½
    print(f'\n{"="*50}')
    print(f'ğŸ“‹ ëª¨ë¸ ë¡œë“œ ìš”ì•½')
    print(f'{"="*50}')
    print(f'  CatBoost : {"âœ… ë¡œë“œë¨" if models.get("catboost_model") else "âŒ ì—†ìŒ"}')
    print(f'  CNN-LSTM : {len(models.get("cnnlstm_models", []))} folds')
    print(f'  PatchTST : {len(models.get("patchtst_models", []))} folds')
    print(f'  Strategy : {models.get("best_strategy", "ë‹¨ìˆœí‰ê· ")}')
    print(f'  Blend W  : {"âœ…" if models.get("blend_weights") else "âŒ"}')
    print(f'{"="*50}')

    return models

log('âœ… ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ')

# ## ğŸ”® 5. ì˜ˆì¸¡ í•¨ìˆ˜ (5-Fold Ensemble + Meta-Learner + Regime)

# ==========================================
# ê°œë³„ ëª¨ë¸ ì˜ˆì¸¡ í•¨ìˆ˜
# ==========================================

def predict_with_catboost(model, X):
    """CatBoost ë‹¨ì¼ ì˜ˆì¸¡: UP í™•ë¥  ë°˜í™˜"""
    try:
        proba = model.predict_proba(X)
        if proba.ndim == 2 and proba.shape[1] >= 2:
            return float(proba[0, 1])
        return float(proba[0])
    except Exception as e:
        log(f'  âš ï¸ CatBoost ì˜ˆì¸¡ ì‹¤íŒ¨: {e}')
        return 0.5

def predict_with_fold_models(models_list, X_seq):
    """
    5-Fold ëª¨ë¸ ì˜ˆì¸¡ í‰ê· 
    X_seq: (seq_len, n_features) numpy array
    """
    if not models_list:
        return 0.5

    probs = []
    for model in models_list:
        try:
            model.eval()
            with torch.no_grad():
                X_tensor = torch.FloatTensor(X_seq).unsqueeze(0).to(config.DEVICE)
                output = model(X_tensor)
                prob = torch.softmax(output, dim=1)[0, 1].item()
                probs.append(prob)
        except Exception as e:
            log(f'  âš ï¸ Fold ëª¨ë¸ ì˜ˆì¸¡ ì‹¤íŒ¨: {e}')
            probs.append(0.5)

    return float(np.mean(probs))

# ==========================================
# Meta Feature ìƒì„± (v7E í•™ìŠµê³¼ ë™ì¼)
# ==========================================

def make_meta_features(p_cat, p_cnn, p_patch):
    """
    Meta-Learnerìš© í”¼ì²˜ ìƒì„± (v7E í•™ìŠµê³¼ ë™ì¼í•œ 10ê°œ í”¼ì²˜)
    [p1, p2, p3, |p1-0.5|, |p2-0.5|, |p3-0.5|, |p1-p2|, |p2-p3|, |p1-p3|, vote_count]
    """
    p1, p2, p3 = np.atleast_1d(p_cat), np.atleast_1d(p_cnn), np.atleast_1d(p_patch)
    return np.column_stack([
        p1, p2, p3,
        np.abs(p1 - 0.5), np.abs(p2 - 0.5), np.abs(p3 - 0.5),
        np.abs(p1 - p2), np.abs(p2 - p3), np.abs(p1 - p3),
        (p1 > 0.5).astype(int) + (p2 > 0.5).astype(int) + (p3 > 0.5).astype(int)
    ])

# ==========================================
# Regime Detection (v7E í•™ìŠµê³¼ ë™ì¼)
# ==========================================

def detect_regime(df_slice):
    """ì‹œì¥ êµ­ë©´ ê°ì§€: bull / bear / high_vol / sideways"""
    if len(df_slice) < 168:
        return 'sideways'

    close_col = 'close' if 'close' in df_slice.columns else 'close_price'
    close = pd.to_numeric(df_slice[close_col], errors='coerce').values
    close = close[~np.isnan(close)]

    if len(close) < 168:
        return 'sideways'

    sma_7d = np.mean(close[-168:])
    sma_30d = np.mean(close[-720:]) if len(close) >= 720 else sma_7d

    vol_col = 'volatility_30' if 'volatility_30' in df_slice.columns else None
    vol = float(df_slice[vol_col].iloc[-1]) if vol_col and pd.notna(df_slice[vol_col].iloc[-1]) else 0.02

    if close[-1] > sma_7d * 1.02 and close[-1] > sma_30d:
        return 'bull'
    elif close[-1] < sma_7d * 0.98 or close[-1] < sma_30d * 0.95:
        return 'bear'
    elif vol > 0.04:
        return 'high_vol'
    else:
        return 'sideways'

def get_regime_weights(regime):
    """Regimeë³„ ëª¨ë¸ ê°€ì¤‘ì¹˜ (cat, cnn, patch)"""
    if regime == 'bull':
        return (0.15, 0.25, 0.60)
    elif regime == 'bear':
        return (0.50, 0.30, 0.20)
    elif regime == 'high_vol':
        return (0.20, 0.55, 0.25)
    else:  # sideways
        return (0.33, 0.34, 0.33)

# ==========================================
# í†µí•© ì•™ìƒë¸” ì˜ˆì¸¡
# ==========================================

def ensemble_predict_v7e(models, X_latest_cat, X_seq_cnn, X_seq_patch, df_recent):
    """
    v7E ì•™ìƒë¸” ì˜ˆì¸¡ íŒŒì´í”„ë¼ì¸:
    1. ê°œë³„ ëª¨ë¸ ì˜ˆì¸¡ (CatBoost + 5-Fold CNN-LSTM + 5-Fold PatchTST)
    2. 3-Level Stacking Meta-Learner
    3. Regime-Based Dynamic Ensemble
    4. ìµœì¢… ì˜ˆì¸¡ + ì‹ ë¢°ë„ + ì˜ˆìƒ ì •í™•ë„
    """
    log('ğŸ”® ì˜ˆì¸¡ ìˆ˜í–‰', important=True)

    individual_predictions = {}

    # 1. ê°œë³„ ëª¨ë¸ ì˜ˆì¸¡
    if models.get('catboost_model') is not None and X_latest_cat is not None:
        prob_cat = predict_with_catboost(models['catboost_model'], X_latest_cat)
    else:
        prob_cat = 0.5
    individual_predictions['catboost'] = prob_cat

    prob_cnn = predict_with_fold_models(models.get('cnnlstm_models', []), X_seq_cnn)
    individual_predictions['cnnlstm'] = prob_cnn

    prob_patch = predict_with_fold_models(models.get('patchtst_models', []), X_seq_patch)
    individual_predictions['patchtst'] = prob_patch

    print(f'\n  ğŸ“Š ê°œë³„ ì˜ˆì¸¡ ê²°ê³¼:')
    print(f'    CatBoost : {prob_cat:.4f} ({"UP" if prob_cat > 0.5 else "DOWN"})')
    print(f'    CNN-LSTM : {prob_cnn:.4f} ({"UP" if prob_cnn > 0.5 else "DOWN"}) [{len(models.get("cnnlstm_models",[]))} folds]')
    print(f'    PatchTST : {prob_patch:.4f} ({"UP" if prob_patch > 0.5 else "DOWN"}) [{len(models.get("patchtst_models",[]))} folds]')

    # 2. dynG ì•™ìƒˆë¸” ì „ëµ (blend_weights + best_strategy ê¸°ë°˜)
    blend_weights = models.get('blend_weights')
    best_strategy = models.get('best_strategy', 'A: Simple Avg')

    # Strategy A: Simple Average
    prob_simple = (prob_cat + prob_cnn + prob_patch) / 3.0

    # Strategy B: Confidence-Weighted
    c1, c2, c3 = abs(prob_cat - 0.5), abs(prob_cnn - 0.5), abs(prob_patch - 0.5)
    total_c = c1 + c2 + c3 + 1e-8
    prob_conf_w = (c1/total_c)*prob_cat + (c2/total_c)*prob_cnn + (c3/total_c)*prob_patch

    # Strategy C: Consensus-Boosted
    votes_up = (prob_cat > 0.5) + (prob_cnn > 0.5) + (prob_patch > 0.5)
    prob_consensus = prob_simple if (votes_up == 3 or votes_up == 0) else prob_conf_w

    # Strategy D: OOF Blend (Ridge)
    if blend_weights and len(blend_weights) >= 3:
        prob_blend = blend_weights[0]*prob_cat + blend_weights[1]*prob_cnn + blend_weights[2]*prob_patch
    else:
        prob_blend = prob_simple

    # dynG Best Strategy ì„ íƒ
    if 'Simple' in best_strategy:
        prob_stacking = prob_simple
    elif 'Confidence' in best_strategy:
        prob_stacking = prob_conf_w
    elif 'Consensus' in best_strategy:
        prob_stacking = prob_consensus
    else:  # OOF Blend (Ridge)
        prob_stacking = prob_blend

    prob_l2 = prob_stacking  # í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€
    print(f'\n  ğŸ§  dynG [{best_strategy}]: {prob_stacking:.4f}')

    # 3. Regime-Based Dynamic Ensemble
    regime = detect_regime(df_recent)
    w = get_regime_weights(regime)
    prob_regime = w[0] * prob_cat + w[1] * prob_cnn + w[2] * prob_patch
    print(f'  ğŸŒ Regime: {regime} | Weights=(cat:{w[0]}, cnn:{w[1]}, patch:{w[2]}) | Prob={prob_regime:.4f}')

    # 4. ìµœì¢… í™•ë¥ : Stacking(60%) + Regime(40%)
    final_prob = 0.6 * prob_stacking + 0.4 * prob_regime
    print(f'  ğŸ“ Final = 0.6*Stacking + 0.4*Regime = {final_prob:.4f}')

    # 5. ì˜ˆì¸¡ + ì‹ ë¢°ë„
    prediction = 1 if final_prob > 0.5 else 0
    confidence = max(final_prob, 1 - final_prob)

    # 6. Confidence -> Predicted Accuracy (ì´ì°¨ ë°©ì •ì‹)
    coeffs = models.get('confidence_coeffs', {'a': 0, 'b': 1, 'c': 0})
    conf_pct = confidence * 100
    predicted_accuracy = coeffs['a'] * conf_pct**2 + coeffs['b'] * conf_pct + coeffs['c']
    predicted_accuracy = np.clip(predicted_accuracy, 50, 100)

    details = {
        'individual_predictions': individual_predictions,
        'individual_avg': float((prob_cat + prob_cnn + prob_patch) / 3),
        'meta_l2_probability': float(prob_l2),
        'meta_stacking_probability': float(prob_stacking),
        'regime': regime,
        'regime_weights': list(w),
        'regime_probability': float(prob_regime),
        'final_probability': float(final_prob),
        'predicted_accuracy_pct': float(predicted_accuracy),
        'used_dynG_strategy': best_strategy,
        'n_cnnlstm_folds': len(models.get('cnnlstm_models', [])),
        'n_patchtst_folds': len(models.get('patchtst_models', [])),
    }

    print(f'\n  {"="*50}')
    print(f'  ğŸ¯ ì˜ˆì¸¡: {"ğŸŸ¢ UP (ìƒìŠ¹)" if prediction == 1 else "ğŸ”´ DOWN (í•˜ë½)"}')
    print(f'  ğŸ“Š ì‹ ë¢°ë„: {confidence:.4f} ({confidence*100:.1f}%)')
    print(f'  ğŸ“ˆ ì˜ˆìƒ ì •í™•ë„: {predicted_accuracy:.1f}%')
    print(f'  {"="*50}')

    return prediction, confidence, details

log('âœ… ì˜ˆì¸¡ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ')

# ## ğŸ“š 6. ì¦ë¶„ í•™ìŠµ & ëª¨ë¸ ì €ì¥

# ==========================================
# ì¦ë¶„ í•™ìŠµ í•¨ìˆ˜ (Fine-tuning)
# ==========================================

class SeqDataset(Dataset):
    """ì‹œí€€ìŠ¤ ë°ì´í„°ì…‹ (v7E í•™ìŠµê³¼ ë™ì¼)"""
    def __init__(self, X, y, seq_len):
        self.X = torch.FloatTensor(X)
        self.y = torch.LongTensor(y)
        self.seq_len = seq_len
    def __len__(self):
        return max(0, len(self.X) - self.seq_len)
    def __getitem__(self, idx):
        return self.X[idx:idx+self.seq_len], self.y[idx+self.seq_len-1]

def incremental_train_catboost(model, X_new, y_new):
    """CatBoost ì¦ë¶„ í•™ìŠµ (init_model ê¸°ë°˜)"""
    if len(X_new) < 10:
        log('  âš ï¸ CatBoost ì¦ë¶„ í•™ìŠµ ìƒëµ: ë°ì´í„° ë¶€ì¡±')
        return model

    split_idx = int(len(X_new) * 0.8)
    X_tr, X_va = X_new[:split_idx], X_new[split_idx:]
    y_tr, y_va = y_new[:split_idx], y_new[split_idx:]

    try:
        model.fit(X_tr, y_tr, eval_set=(X_va, y_va), init_model=model, verbose=False)
        val_acc = accuracy_score(y_va, model.predict(X_va))
        log(f'  âœ… CatBoost ì¦ë¶„ í•™ìŠµ ì™„ë£Œ: Val Acc={val_acc:.4f}')
    except Exception as e:
        log(f'  âš ï¸ CatBoost ì¦ë¶„ í•™ìŠµ ì‹¤íŒ¨: {e}')
    return model

def incremental_train_deep_model(model, X_seq, y_seq, model_name, epochs=10, batch_size=32):
    """ë”¥ëŸ¬ë‹ ëª¨ë¸ ì¦ë¶„ í•™ìŠµ (CNN-LSTM / PatchTST)"""
    if len(X_seq) < config.PRIMARY_SEQUENCE_LENGTH + 5:
        log(f'  âš ï¸ {model_name} ì¦ë¶„ í•™ìŠµ ìƒëµ: ì‹œí€€ìŠ¤ ë°ì´í„° ë¶€ì¡±')
        return model

    dataset = SeqDataset(X_seq, y_seq, config.PRIMARY_SEQUENCE_LENGTH)
    if len(dataset) < 5:
        log(f'  âš ï¸ {model_name} ì¦ë¶„ í•™ìŠµ ìƒëµ: ì‹œí€€ìŠ¤ ë¶€ì¡± ({len(dataset)}ê°œ)')
        return model

    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)
    criterion = FocalLoss(config.FOCAL_GAMMA, config.FOCAL_ALPHA, config.LABEL_SMOOTHING)

    model.train()
    for epoch in range(epochs):
        for batch_X, batch_y in dataloader:
            batch_X, batch_y = batch_X.to(config.DEVICE), batch_y.to(config.DEVICE)
            optimizer.zero_grad()
            loss = criterion(model(batch_X), batch_y)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()

    model.eval()
    log(f'  âœ… {model_name} ì¦ë¶„ í•™ìŠµ ì™„ë£Œ ({epochs} epochs, {len(dataset)} sequences)')
    return model

def run_incremental_training(models, df_gap, features):
    """ì „ì²´ ëª¨ë¸ ì¦ë¶„ í•™ìŠµ ìˆ˜í–‰"""
    if df_gap is None or len(df_gap) < 30:
        log('âš ï¸ ì¦ë¶„ í•™ìŠµ ìƒëµ: ê°­ ë°ì´í„° ë¶€ì¡±')
        return False

    log('ğŸ“š ì¦ë¶„ í•™ìŠµ (Fine-tuning) ì‹œì‘', important=True)
    log(f'  ê°­ ë°ì´í„°: {len(df_gap):,} rows')

    trained = False
    df_gap_valid = df_gap.dropna(subset=['target'])

    # CatBoost ì¦ë¶„ í•™ìŠµ
    if models.get('catboost_model') is not None:
        # ëˆ„ë½ í”¼ì²˜ 0ìœ¼ë¡œ ì±„ì›€
        for f in features['catboost']:
            if f not in df_gap_valid.columns:
                df_gap_valid[f] = 0
        if len(df_gap_valid) >= 10:
            X_new = models['scaler_catboost'].transform(df_gap_valid[features['catboost']].fillna(0).values)
            y_new = df_gap_valid['target'].values.astype(int)
            models['catboost_model'] = incremental_train_catboost(models['catboost_model'], X_new, y_new)
            trained = True

    # CNN-LSTM 5-Fold ì¦ë¶„ í•™ìŠµ
    if models.get('cnnlstm_models'):
        # ëˆ„ë½ í”¼ì²˜ 0ìœ¼ë¡œ ì±„ì›€
        for f in features['cnnlstm']:
            if f not in df_gap_valid.columns:
                df_gap_valid[f] = 0
        X_cnn = models['scaler_cnnlstm'].transform(df_gap_valid[features['cnnlstm']].fillna(0).values)
        y_cnn = df_gap_valid['target'].values.astype(int)
        for i, model in enumerate(models.get('cnnlstm_models', [])):
            models['cnnlstm_models'][i] = incremental_train_deep_model(model, X_cnn, y_cnn, f'CNN-LSTM_f{i}', epochs=10)
        trained = True

    # PatchTST 5-Fold ì¦ë¶„ í•™ìŠµ
    if models.get('patchtst_models'):
        # ëˆ„ë½ í”¼ì²˜ 0ìœ¼ë¡œ ì±„ì›€
        for f in features['patchtst']:
            if f not in df_gap_valid.columns:
                df_gap_valid[f] = 0
        X_patch = models['scaler_patchtst'].transform(df_gap_valid[features['patchtst']].fillna(0).values)
        y_patch = df_gap_valid['target'].values.astype(int)
        for i, model in enumerate(models.get('patchtst_models', [])):
            models['patchtst_models'][i] = incremental_train_deep_model(model, X_patch, y_patch, f'PatchTST_f{i}', epochs=10)
        trained = True

    if trained:
        log('âœ… ì¦ë¶„ í•™ìŠµ ì™„ë£Œ', important=True)
    return trained
# ==========================================
# ëª¨ë¸ ì €ì¥ í•¨ìˆ˜
# ==========================================

def save_updated_models(models, last_data_date=None):
    """ì—…ë°ì´íŠ¸ëœ ëª¨ë¸ ì €ì¥"""
    log('ğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...')

    # CatBoost
    if models.get('catboost_model') is not None:
        cat_path = os.path.join(MODEL_DIR, 'catboost_model_v7e.cbm')
        models['catboost_model'].save_model(cat_path)
        log(f'  CatBoost: {cat_path}')

    # CNN-LSTM (5-Fold)
    for i, model in enumerate(models.get('cnnlstm_models', [])):
        path = os.path.join(MODEL_DIR, f'cnnlstm_f{i}.pth')
        torch.save({'model_state_dict': model.state_dict()}, path)
    log(f'  CNN-LSTM: {len(models.get("cnnlstm_models", []))} folds ì €ì¥')

    # PatchTST (5-Fold)
    for i, model in enumerate(models.get('patchtst_models', [])):
        path = os.path.join(MODEL_DIR, f'patchtst_f{i}.pth')
        torch.save({'model_state_dict': model.state_dict()}, path)
    log(f'  PatchTST: {len(models.get("patchtst_models", []))} folds ì €ì¥')

    # ë©”íƒ€ë°ì´í„°
    metadata = {
        'last_updated': datetime.now(timezone.utc).isoformat(),
        'last_data_date': pd.to_datetime(last_data_date).strftime('%Y-%m-%d %H:%M:%S') if last_data_date is not None else None,
        'model_version': 'v7E',
        'n_folds': config.N_FOLDS,
        'models': ['catboost', 'cnnlstm_5fold', 'patchtst_5fold', 'meta_l2', 'meta_l3']
    }
    with open(os.path.join(MODEL_DIR, 'model_metadata.json'), 'w') as f:
        json.dump(metadata, f, indent=2)

    log('âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ')

# ==========================================
# ì˜ˆì¸¡ ê²°ê³¼ Supabase ì €ì¥
# ==========================================

def save_prediction_to_supabase(prediction_date, prediction, confidence, model_details,
                                 current_price=None, current_price_krw=None):
    """ì˜ˆì¸¡ ê²°ê³¼ë¥¼ Supabase predictions í…Œì´ë¸”ì— ì €ì¥"""
    try:
        if current_price is None:
            current_price = get_realtime_btc_price_usd() or 0
        if current_price_krw is None:
            current_price_krw = get_krw_bitcoin_price() or 0

        data = {
            'date': prediction_date.strftime('%Y-%m-%d %H:%M:%S+00:00'),
            'predicted_price': float(current_price),
            'predicted_price_krw': float(current_price_krw) if current_price_krw else None,
            'direction': 'UP' if prediction == 1 else 'DOWN',
            'confidence_score': float(confidence),
            'model_breakdown': json.dumps(model_details, ensure_ascii=False, default=str)
        }

        response = supabase.table('predictions').upsert(data, on_conflict='date').execute()
        log(f'âœ… ì˜ˆì¸¡ ì €ì¥ ì™„ë£Œ: {prediction_date.strftime("%Y-%m-%d %H:%M")}')
        return response
    except Exception as e:
        log(f'âŒ ì˜ˆì¸¡ ì €ì¥ ì‹¤íŒ¨: {e}')
        return None

log('âœ… ì¦ë¶„ í•™ìŠµ / ëª¨ë¸ ì €ì¥ / ì˜ˆì¸¡ ì €ì¥ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ')

# ## ğŸš€ 7. ë©”ì¸ íŒŒì´í”„ë¼ì¸

# ==========================================
# ì–´ì œ ì˜ˆì¸¡ ê²€ì¦ í•¨ìˆ˜
# ==========================================

def validate_yesterday_prediction():
    """ì–´ì œ ì˜ˆì¸¡ ê²°ê³¼ ê²€ì¦"""
    try:
        now_utc = datetime.now(timezone.utc)
        yesterday = (now_utc - timedelta(hours=48)).strftime('%Y-%m-%d')

        result = supabase.table('predictions').select('*').gte('date', yesterday).order('date', desc=True).limit(5).execute()
        if not result.data:
            log('  ì–´ì œ ì˜ˆì¸¡ ë°ì´í„° ì—†ìŒ')
            return

        print(f'\n  ìµœê·¼ ì˜ˆì¸¡ ê¸°ë¡:')
        for pred in result.data:
            pred_date = pred.get('date', '')
            direction = pred.get('direction', '?')
            confidence = pred.get('confidence_score', 0)
            actual = pred.get('actual_result') or pred.get('actual_direction')
            if actual and actual == direction:
                status = 'âœ…'
            elif actual:
                status = 'âŒ'
            else:
                status = 'â³'
            conf_str = f'{confidence:.1%}' if confidence else 'N/A'
            print(f'    {status} {pred_date[:16]} | ì˜ˆì¸¡:{direction} | ì‹¤ì œ:{actual or "ëŒ€ê¸°ì¤‘"} | ì‹ ë¢°ë„:{conf_str}')

    except Exception as e:
        log(f'  âš ï¸ ê²€ì¦ ì‹¤íŒ¨: {e}')

# ==========================================
# ê³¼ê±° ì˜ˆì¸¡ ê²€ì¦ (01a_validate_past_predictions ë¡œì§)
# - is_correctê°€ NULLì¸ ì˜ˆì¸¡ì— ëŒ€í•´ 24ì‹œê°„ ë’¤ ì‹¤ì œ ê°€ê²©ìœ¼ë¡œ ê²€ì¦
# - Binance/Upbit 1ë¶„ ìº”ë“¤ APIë¡œ ì •í™•í•œ ì‹œì  ê°€ê²© ì¡°íšŒ
# ==========================================

def get_historical_usd_price(target_dt):
    """Binance Klines APIë¥¼ ì‚¬ìš©í•˜ì—¬ íŠ¹ì • ì‹œì ì˜ BTC/USDT ì¢…ê°€ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    try:
        timestamp_ms = int(target_dt.timestamp() * 1000)
        url = "https://api.binance.com/api/v3/klines"
        params = {"symbol": "BTCUSDT", "interval": "1m", "endTime": timestamp_ms, "limit": 1}
        response = requests.get(url, params=params, timeout=10)
        response.raise_for_status()
        data = response.json()
        if data and len(data) > 0:
            return float(data[0][4])
        return None
    except Exception as e:
        log(f'  USD ê°€ê²© ì¡°íšŒ ì‹¤íŒ¨: {e}')
        return None

def get_historical_krw_price(target_dt):
    """Upbit ìº”ë“¤ APIë¥¼ ì‚¬ìš©í•˜ì—¬ íŠ¹ì • ì‹œì ì˜ BTC/KRW ì¢…ê°€ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    try:
        kst_dt = target_dt.astimezone(timezone(timedelta(hours=9)))
        to_str = kst_dt.strftime('%Y-%m-%dT%H:%M:%S+09:00')
        url = "https://api.upbit.com/v1/candles/minutes/1"
        params = {"market": "KRW-BTC", "to": to_str, "count": 1}
        response = requests.get(url, params=params, headers={"accept": "application/json"}, timeout=10)
        response.raise_for_status()
        data = response.json()
        if data and len(data) > 0:
            return float(data[0]['trade_price'])
        return None
    except Exception as e:
        log(f'  KRW ê°€ê²© ì¡°íšŒ ì‹¤íŒ¨: {e}')
        return None

def validate_past_predictions():
    """
    is_correctê°€ NULLì¸ ì˜ˆì¸¡ì— ëŒ€í•´ ì‹¤ì œ ê°€ê²©ìœ¼ë¡œ ê²€ì¦ ë° ì—…ë°ì´íŠ¸.
    [ì¤‘ìš”] 31FHëŠ” dateì— 'ì˜ˆì¸¡ ëŒ€ìƒ ì‹œì '(24h í›„)ì„ ì €ì¥í•¨.
    â†’ target_dt = pred_dt (date ìì²´ê°€ ê²€ì¦ ëŒ€ìƒ ì‹œì , +24h í•˜ë©´ ì•ˆ ë¨!)
    """
    log('ğŸ”„ DBì—ì„œ is_correctê°€ NULLì¸ ì˜ˆì¸¡ ë°ì´í„°ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤...')
    resp = supabase.table("predictions").select("*").is_("is_correct", "null").execute()
    if not resp.data:
        log('âœ… ê²€ì¦í•  ëˆ„ë½ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.')
        return
    records = resp.data
    log(f'ì´ {len(records)}ê°œì˜ ë¯¸ê²° ì˜ˆì¸¡ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.')
    current_utc = datetime.now(timezone.utc)
    updated_count = 0
    for row in records:
        pred_date_str = row['date']
        try:
            pred_dt = pd.to_datetime(pred_date_str)
            if pred_dt.tzinfo is None:
                pred_dt = pred_dt.replace(tzinfo=timezone.utc)
            # date = ì˜ˆì¸¡ ëŒ€ìƒ ì‹œì  (31FH: now+24hë¡œ ì €ì¥ë¨) â†’ ì´ ì‹œì ì˜ ê°€ê²©ì´ actual_price
            target_dt = pred_dt  # +24h í•˜ì§€ ì•ŠìŒ! date ìì²´ê°€ ëª©í‘œ ì‹œì 
            if current_utc < target_dt:
                log(f'â³ {pred_date_str} ì˜ˆì¸¡ì€ ì•„ì§ ëª©í‘œ ì‹œì ì´ ë„ë˜í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. (ëª©í‘œ: {target_dt.strftime("%m-%d %H:%M")})')
                continue
            log(f'ğŸ” {pred_date_str} ì˜ˆì¸¡ ê²€ì¦ ì¤‘ ...')
            base_price = row.get('predicted_price')
            if not base_price:
                log('  âŒ ê¸°ì¤€ ê°€ê²©(predicted_price)ì´ ì—†ì–´ ê³„ì‚° ìš°íšŒ')
                continue
            actual_usd = get_historical_usd_price(target_dt)
            if not actual_usd:
                log('  âš ï¸ Binance 1m API ì‹¤íŒ¨ â†’ features_master/Binance 1d í´ë°±')
                actual_usd = _get_actual_price_from_features_master(target_dt)
                if not actual_usd:
                    actual_usd = _get_actual_price_from_binance(target_dt)
            actual_krw = get_historical_krw_price(target_dt) if actual_usd else None
            if not actual_krw and actual_usd:
                actual_krw = estimate_krw_price_from_usd(actual_usd)
            time.sleep(0.2)
            if not actual_usd:
                log('  âŒ ì‹¤ì œ ê°€ê²©ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (Binance 451/ì§€ì—­ì œí•œ ì‹œ features_master í™•ì¸)')
                continue
            change_pct = ((actual_usd - base_price) / base_price) * 100
            actual_dir = "UP" if change_pct > 0 else "DOWN"
            predicted_dir = row['direction']
            is_correct = (actual_dir == predicted_dir or
                         (actual_dir == "UP" and predicted_dir in ["ìƒìŠ¹", "UP", "1"]) or
                         (actual_dir == "DOWN" and predicted_dir in ["í•˜ë½", "DOWN", "0"]))
            update_data = {
                'actual_price': actual_usd,
                'actual_price_krw': actual_krw if actual_krw else row.get('actual_price_krw'),
                'price_change_pct': round(change_pct, 2),
                'actual_result': actual_dir,
                'is_correct': is_correct
            }
            date_key = _format_date_for_eq(pred_date_str)
            supabase.table("predictions").update(update_data).eq("date", date_key).execute()
            log(f'  âœ… ì—…ë°ì´íŠ¸ ì™„ë£Œ! ì˜ˆì¸¡:{predicted_dir} | ì‹¤ì œ:{actual_dir} | ì •ë‹µì—¬ë¶€:{is_correct} (ë³€ë™:{change_pct:.2f}%)')
            updated_count += 1
        except Exception as e:
            log(f'  âŒ ì—ëŸ¬ ë°œìƒ ({pred_date_str}): {e}')
    log(f'ğŸ‰ ì´ {updated_count}ê°œì˜ ì˜ˆì¸¡ ë°ì´í„°ë¥¼ ê²€ì¦ ë° ì—…ë°ì´íŠ¸ í–ˆìŠµë‹ˆë‹¤.')

# ==========================================
# actual_price / is_correct ë°±í•„ í•¨ìˆ˜
# ==========================================

def _format_date_for_eq(date_val):
    """Supabase .eq() í•„í„°ìš© date í¬ë§· í†µì¼ (insert ì‹œ ì‚¬ìš©í•œ í˜•ì‹ê³¼ ë™ì¼)"""
    dt = pd.to_datetime(date_val)
    if dt.tzinfo is None:
        dt = dt.replace(tzinfo=timezone.utc)
    return dt.strftime('%Y-%m-%d %H:%M:%S+00:00')

def estimate_krw_price_from_usd(usd_price, exchange_rate=1450):
    """USD ê°€ê²©ì„ KRWë¡œ í™˜ì‚° (Backfillìš©)"""
    return usd_price * exchange_rate

def _get_actual_price_from_features_master(pred_date):
    """features_masterì—ì„œ í•´ë‹¹ ì‹œì  ë˜ëŠ” ì§ì „ì˜ close ê°€ê²© ì¡°íšŒ (ëª©í‘œ ì‹œì ì— ê°€ê¹Œìš´ ë°ì´í„°)"""
    try:
        ts_str = pd.to_datetime(pred_date).strftime('%Y-%m-%d %H:%M:%S')
        price_query = supabase.table('features_master').select('close').lte('date', ts_str).order('date', desc=True).limit(1).execute()
        if price_query.data and price_query.data[0].get('close'):
            return float(price_query.data[0]['close'])
        date_str = pd.to_datetime(pred_date).strftime('%Y-%m-%d')
        next_date_str = (pd.to_datetime(pred_date) + timedelta(days=1)).strftime('%Y-%m-%d')
        price_query = supabase.table('features_master').select('close').gte('date', date_str).lt('date', next_date_str).order('date', desc=True).limit(1).execute()
        if price_query.data and price_query.data[0].get('close'):
            return float(price_query.data[0]['close'])
    except Exception:
        pass
    return None

def _get_actual_price_from_binance(pred_date):
    """Binance APIë¡œ í•´ë‹¹ ë‚ ì§œ ì¢…ê°€ ì¡°íšŒ (features_master ì—†ì„ ë•Œ í´ë°±)"""
    try:
        ts = int(pd.Timestamp(pred_date).timestamp() * 1000)
        r = requests.get(f'https://api.binance.com/api/v3/klines?symbol=BTCUSDT&interval=1d&startTime={ts}&limit=1', timeout=10)
        if r.status_code == 200 and r.json():
            return float(r.json()[0][4])
    except Exception:
        pass
    return None

def backfill_missing_actual_prices():
    """actual_priceê°€ NULLì¸ ì˜ˆì¸¡ì— ëŒ€í•´ features_master ë˜ëŠ” Binanceì—ì„œ ê°€ê²© ì±„ìš°ê¸°"""
    try:
        log('  ğŸ”„ ëˆ„ë½ëœ actual_price ì¼ê´„ ì±„ìš°ê¸°...')
        response = supabase.table('predictions').select('*').is_('actual_price', 'null').execute()
        if not response.data:
            log('  âœ… ëª¨ë“  actual_priceê°€ ì´ë¯¸ ì±„ì›Œì ¸ ìˆìŠµë‹ˆë‹¤.')
            backfill_missing_actual_price_krw()
            backfill_validation_fields()
            return 0
        log(f'  ğŸ“‹ actual_price NULLì¸ ë ˆì½”ë“œ {len(response.data)}ê°œ ë°œê²¬')
        updated = 0
        for record in response.data:
            try:
                pred_date = pd.to_datetime(record['date'])
                actual_usd = _get_actual_price_from_features_master(pred_date)
                if actual_usd is None:
                    actual_usd = _get_actual_price_from_binance(pred_date)
                if actual_usd and actual_usd > 0:
                    actual_krw = estimate_krw_price_from_usd(actual_usd)
                    date_key = _format_date_for_eq(record['date'])
                    supabase.table('predictions').update({'actual_price': actual_usd, 'actual_price_krw': actual_krw}).eq('date', date_key).execute()
                    log(f'     {pred_date.strftime("%Y-%m-%d")}: ${actual_usd:,.2f} / â‚©{actual_krw:,.0f}')
                    updated += 1
                else:
                    log(f'     âš ï¸ {pred_date.strftime("%Y-%m-%d")}: features_master/Binanceì—ì„œ ê°€ê²© ì¡°íšŒ ì‹¤íŒ¨')
            except Exception as e:
                log(f'     âš ï¸ {record.get("date", "?")}: {e}')
        if updated > 0:
            log(f'  âœ… {updated}ê°œ actual_price ì—…ë°ì´íŠ¸ ì™„ë£Œ')
        backfill_missing_actual_price_krw()
        backfill_validation_fields()
        return updated
    except Exception as e:
        log(f'  âŒ actual_price ë°±í•„ ì‹¤íŒ¨: {e}')
        import traceback
        traceback.print_exc()
        return 0

def backfill_missing_actual_price_krw():
    """actual_priceëŠ” ìˆì§€ë§Œ actual_price_krwê°€ NULLì¸ ë ˆì½”ë“œ ì±„ìš°ê¸°"""
    try:
        response = supabase.table('predictions').select('*').not_.is_('actual_price', 'null').is_('actual_price_krw', 'null').execute()
        if not response.data:
            return 0
        for record in response.data:
            try:
                actual_krw = estimate_krw_price_from_usd(float(record['actual_price']))
                date_key = _format_date_for_eq(record['date'])
                supabase.table('predictions').update({'actual_price_krw': actual_krw}).eq('date', date_key).execute()
            except Exception:
                pass
        return len(response.data)
    except Exception:
        return 0

def backfill_validation_fields():
    """actual_priceëŠ” ìˆì§€ë§Œ is_correctê°€ NULLì¸ ë ˆì½”ë“œ ì±„ìš°ê¸°"""
    try:
        response = supabase.table('predictions').select('*').not_.is_('actual_price', 'null').is_('is_correct', 'null').execute()
        if not response.data:
            log('  âœ… ëª¨ë“  ê²€ì¦ í•„ë“œê°€ ì´ë¯¸ ì±„ì›Œì ¸ ìˆìŠµë‹ˆë‹¤.')
            return 0
        log(f'  ğŸ”„ ê²€ì¦ í•„ë“œ ì±„ìš°ê¸° ({len(response.data)}ê°œ)...')
        updated = 0
        for record in response.data:
            try:
                actual_price = float(record['actual_price'])
                predicted_price = record.get('predicted_price') or 0
                pred_direction = record.get('direction', 'UNKNOWN')
                if predicted_price > 0:
                    price_change_pct = ((actual_price - predicted_price) / predicted_price) * 100
                    actual_direction = 'UP' if actual_price > predicted_price else 'DOWN'
                    # direction ì •ê·œí™”: ìƒìŠ¹/UP/1 â†’ UP, í•˜ë½/DOWN/0 â†’ DOWN
                    pred_norm = 'UP' if pred_direction in ['ìƒìŠ¹', 'UP', '1'] else ('DOWN' if pred_direction in ['í•˜ë½', 'DOWN', '0'] else pred_direction)
                    is_correct = (pred_norm == actual_direction)
                    update_data = {
                        'actual_result': actual_direction,
                        'is_correct': is_correct,
                        'price_change_pct': round(price_change_pct, 2)
                    }
                    date_key = _format_date_for_eq(record['date'])
                    result = supabase.table('predictions').update(update_data).eq('date', date_key).execute()
                    if not result.data:
                        log(f'     âš ï¸ ì—…ë°ì´íŠ¸ ë§¤ì¹­ ì‹¤íŒ¨ (date={date_key})')
                    status = 'âœ…' if is_correct else 'âŒ'
                    log(f'     {pd.to_datetime(record["date"]).strftime("%Y-%m-%d")}: {pred_direction} vs {actual_direction} {status}')
                    updated += 1
            except Exception as e:
                log(f'     âš ï¸ {record.get("date", "?")}: {e}')
        if updated > 0:
            log(f'  âœ… {updated}ê°œ is_correct ì—…ë°ì´íŠ¸ ì™„ë£Œ')
        return updated
    except Exception as e:
        log(f'  âŒ ê²€ì¦ í•„ë“œ ë°±í•„ ì‹¤íŒ¨: {e}')
        return 0

# ==========================================
# ë©”ì¸ íŒŒì´í”„ë¼ì¸
# ==========================================

def run_daily_pipeline():
    """
    ì¼ì¼ ì˜ˆì¸¡ íŒŒì´í”„ë¼ì¸ (v7E)

    Step 1: ë°ì´í„° ë¡œë“œ
    Step 2: ì–´ì œ ì˜ˆì¸¡ ê²€ì¦
    Step 3: ëª¨ë¸ ë¡œë“œ
    Step 4: ì˜ˆì¸¡ ë°ì´í„° ì¤€ë¹„
    Step 5: ì•™ìƒë¸” ì˜ˆì¸¡
    Step 6: ì¦ë¶„ í•™ìŠµ (ê°­ ë°ì´í„° ìˆì„ ì‹œ)
    Step 7: ì˜ˆì¸¡ ì €ì¥
    Step 8: ëª¨ë¸ ì €ì¥
    """
    pipeline_start = datetime.now(KST)
    print(f'\n{"#"*60}')
    print(f'ğŸš€ v7E Daily Prediction Pipeline ì‹œì‘')
    print(f'   ì‹œì‘ ì‹œê°„: {pipeline_start.strftime("%Y-%m-%d %H:%M:%S")} KST')
    print(f'{"#"*60}')

    try:
        # ============================================================
        # Step 0: ê²½ë¡œ ê°•ì œ ì„¤ì • (ìºì‹œ ë¬¸ì œ ë°©ì§€)
        # ============================================================
        global MODEL_DIR, PROJECT_ROOT
        if IS_COLAB:
            PROJECT_ROOT = '/content/drive/MyDrive/2526Winter_Sideproject'
            MODEL_DIR = os.path.join(PROJECT_ROOT, 'models', 'production', 'v7E_production_highAccuracy_dynH')
        log(f'  MODEL_DIR: {MODEL_DIR}')
        log(f'  íŒŒì¼ í™•ì¸: {os.listdir(MODEL_DIR) if os.path.exists(MODEL_DIR) else "í´ë” ì—†ìŒ!"}')
        
        # ============================================================
        # Step 0.5: ê³¼ê±° ì˜ˆì¸¡ ê²€ì¦ (01a ë¡œì§) + actual_price/is_correct ë°±í•„
        # ============================================================
        log('Step 0.5: ê³¼ê±° ì˜ˆì¸¡ ê²€ì¦ ë° ë°±í•„ (actual_price, is_correct)', important=True)
        validate_past_predictions()  # 01a: is_correct NULL â†’ 24h ë’¤ 1ë¶„ìº”ë“¤ë¡œ ì •í™• ê²€ì¦
        backfill_missing_actual_prices()  # actual_price NULL â†’ features_master/Binance í´ë°±
        
        # ============================================================
        # Step 1: ë°ì´í„° ë¡œë“œ (29F ë¡œì§ê³¼ ë™ì¼)
        # ============================================================
        log('Step 1: ë°ì´í„° ë¡œë“œ', important=True)
        
        # ìµœì‹  ë‚ ì§œ í™•ì¸
        latest_date = get_latest_date_from_supabase()
        if latest_date is None:
            raise ValueError('Supabaseì—ì„œ ìµœì‹  ë‚ ì§œë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤')
        log(f'  ìµœì‹  ë°ì´í„° ë‚ ì§œ: {latest_date}')
        
        # -------------------------------------------------------
        # ë°ì´í„° ë¡œë“œ ì „ëµ:
        #   - CatBoost í•™ìŠµìš©: ìµœê·¼ 4000ì‹œê°„ (~167ì¼, ~5.5ê°œì›”)
        #   - ë”¥ëŸ¬ë‹ ì˜ˆì¸¡ìš©: ê·¸ ì¤‘ ë§ˆì§€ë§‰ 72ì‹œê°„ ì‹œí€€ìŠ¤ ì‚¬ìš©
        #   - íƒ€ì„ì•„ì›ƒ ë°©ì§€: date í•„í„° + 1000í–‰ ë°°ì¹˜ ë¡œë“œ
        # -------------------------------------------------------
        CATBOOST_TRAIN_HOURS = 4000  # CatBoost í•™ìŠµì— ì¶©ë¶„í•œ ê¸°ê°„
        catboost_start = latest_date - timedelta(hours=CATBOOST_TRAIN_HOURS)
        log(f'  ë°ì´í„° ë¡œë“œ ë²”ìœ„: {catboost_start} ~ {latest_date} ({CATBOOST_TRAIN_HOURS}ì‹œê°„, ~{CATBOOST_TRAIN_HOURS//24}ì¼)')
        
        # Supabaseì—ì„œ ë‚ ì§œ í•„í„°ë¡œ ë¡œë“œ (íƒ€ì„ì•„ì›ƒ ë°©ì§€)
        all_rows, offset = [], 0
        start_str = catboost_start.strftime('%Y-%m-%d %H:%M:%S')
        end_str = latest_date.strftime('%Y-%m-%d %H:%M:%S')
        
        while True:
            result = supabase.table('features_master').select('*').gte('date', start_str).lte('date', end_str).order('date').range(offset, offset + 999).execute()
            if not result.data:
                break
            all_rows.extend(result.data)
            offset += len(result.data)
            log(f'    ë°°ì¹˜ ë¡œë“œ: {len(all_rows):,} rows...')
            if len(result.data) < 1000:
                break
        
        df_features = pd.DataFrame(all_rows)
        df_features['date'] = pd.to_datetime(df_features['date'])
        log(f'  features_master: {len(df_features):,} rows')
        
        # ê°ì„± ë°ì´í„° ë³‘í•©
        df_sent = fetch_sentiment_data()
        if not df_sent.empty:
            for col in ['sentiment_score', 'impact_score']:
                if col in df_features.columns:
                    df_features = df_features.drop(columns=[col], errors='ignore')
            df_features = pd.merge(df_features, df_sent, on='date', how='left')
        
        df_features['sentiment_score'] = df_features.get('sentiment_score', 0).fillna(0)
        df_features['impact_score'] = df_features.get('impact_score', 0.5).fillna(0.5)
        
        # close ì»¬ëŸ¼ í†µì¼
        if 'close' not in df_features.columns and 'close_price' in df_features.columns:
            df_features['close'] = df_features['close_price']
        
        # target ìƒì„±
        df_features['close'] = pd.to_numeric(df_features['close'], errors='coerce')
        df_features['target'] = (df_features['close'].shift(-config.PREDICTION_HORIZON) > df_features['close']).astype(int)
        
        df_full = df_features.sort_values('date').reset_index(drop=True)
        log(f'âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {len(df_full):,} rows')


        # On-the-fly íŒŒìƒ í”¼ì²˜ ìƒì„± (Kaggle v7E í•™ìŠµ ì‹œ ìƒì„±ëœ 11ê°œ)
        log('ğŸ“ On-the-fly íŒŒìƒ í”¼ì²˜ ìƒì„± ì¤‘...')
        df_full = add_on_the_fly_features(df_full)
        
        # ============================================================
        # Step 2: ì–´ì œ ì˜ˆì¸¡ ê²€ì¦
        # ============================================================
        log('Step 2: ì–´ì œ ì˜ˆì¸¡ ê²€ì¦', important=True)
        validate_yesterday_prediction()

        # ============================================================
        # Step 3: ëª¨ë¸ ë¡œë“œ
        # ============================================================
        models = load_all_models(df_full=df_full)
        features = models['features']

        # ============================================================
        # Step 4: ì˜ˆì¸¡ ë°ì´í„° ì¤€ë¹„
        # ============================================================
        log('Step 4: ì˜ˆì¸¡ ë°ì´í„° ì¤€ë¹„', important=True)

        min_rows = config.PRIMARY_SEQUENCE_LENGTH * 3
        df_recent = df_full.tail(max(min_rows, 300)).copy()
        log(f'  ì˜ˆì¸¡ìš© ìµœê·¼ ë°ì´í„°: {len(df_recent)} rows')

        # CatBoost: ë§ˆì§€ë§‰ í–‰ì˜ í”¼ì²˜ë¡œ ì˜ˆì¸¡
        # CatBoost: ëˆ„ë½ í”¼ì²˜ 0ìœ¼ë¡œ ì±„ì›€
        for f in features["catboost"]:
            if f not in df_recent.columns:
                df_recent[f] = 0
        X_cat_all = df_recent[features["catboost"]].fillna(0).values
        X_cat_scaled = models['scaler_catboost'].transform(X_cat_all)
        X_latest_cat = X_cat_scaled[[-1]]
        log(f'  CatBoost ì…ë ¥: {X_latest_cat.shape} ({len(features["catboost"])} features)')

        # CNN-LSTM: ë§ˆì§€ë§‰ seq_len í–‰ì˜ ì‹œí€€ìŠ¤
        # CNN-LSTM: ëˆ„ë½ í”¼ì²˜ 0ìœ¼ë¡œ ì±„ì›€
        for f in features["cnnlstm"]:
            if f not in df_recent.columns:
                df_recent[f] = 0
        X_cnn_all = df_recent[features["cnnlstm"]].fillna(0).values
        X_cnn_scaled = models['scaler_cnnlstm'].transform(X_cnn_all)
        X_seq_cnn = X_cnn_scaled[-config.PRIMARY_SEQUENCE_LENGTH:]
        log(f'  CNN-LSTM ì…ë ¥: {X_seq_cnn.shape}')

        # PatchTST: ë§ˆì§€ë§‰ seq_len í–‰ì˜ ì‹œí€€ìŠ¤
        # PatchTST: ëˆ„ë½ í”¼ì²˜ 0ìœ¼ë¡œ ì±„ì›€
        for f in features["patchtst"]:
            if f not in df_recent.columns:
                df_recent[f] = 0
        X_patch_all = df_recent[features["patchtst"]].fillna(0).values
        X_patch_scaled = models['scaler_patchtst'].transform(X_patch_all)
        X_seq_patch = X_patch_scaled[-config.PRIMARY_SEQUENCE_LENGTH:]
        log(f'  PatchTST ì…ë ¥: {X_seq_patch.shape}')

        # ============================================================
        # Step 5: ì•™ìƒë¸” ì˜ˆì¸¡
        # ============================================================
        prediction, confidence, model_details = ensemble_predict_v7e(
            models, X_latest_cat, X_seq_cnn, X_seq_patch, df_recent
        )

        # ============================================================
        # Step 6: ì¦ë¶„ í•™ìŠµ
        # ============================================================
        metadata = get_model_metadata()
        training_performed = False

        if metadata and metadata.get('last_data_date'):
            last_train_date = pd.to_datetime(metadata['last_data_date'])
            df_gap = df_full[df_full['date'] > last_train_date].copy()
            df_gap = df_gap.dropna(subset=['target'])

            if len(df_gap) >= 30:
                log(f'  ê°­ ë°ì´í„°: {len(df_gap)} rows (ë§ˆì§€ë§‰ í•™ìŠµ: {last_train_date})')
                training_performed = run_incremental_training(models, df_gap, features)
            else:
                log(f'  ì¦ë¶„ í•™ìŠµ ìƒëµ: ê°­ ë°ì´í„° {len(df_gap)}í–‰ (ìµœì†Œ 30 í•„ìš”)')
        else:
            log('  ëª¨ë¸ ë©”íƒ€ë°ì´í„° ì—†ìŒ - ì¦ë¶„ í•™ìŠµ ìƒëµ')

        # ============================================================
        # Step 7: ì˜ˆì¸¡ ì €ì¥
        # ============================================================
        log('Step 7: ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥', important=True)

        now_utc = datetime.now(timezone.utc)
        prediction_target = now_utc + timedelta(hours=24)
        current_usd = get_realtime_btc_price_usd()
        current_krw = get_krw_bitcoin_price()

        save_prediction_to_supabase(
            prediction_target, prediction, confidence, model_details,
            current_price=current_usd, current_price_krw=current_krw
        )

        # ============================================================
        # Step 8: ëª¨ë¸ ì €ì¥ (í•™ìŠµ ìˆ˜í–‰ ì‹œ)
        # ============================================================
        if training_performed:
            log('Step 8: ëª¨ë¸ ì €ì¥', important=True)
            save_updated_models(models, last_data_date=latest_date)
        else:
            log('  í•™ìŠµ ë¯¸ìˆ˜í–‰ - ëª¨ë¸ ì €ì¥ ìƒëµ')

        # ============================================================
        # ìµœì¢… ê²°ê³¼ ì¶œë ¥
        # ============================================================
        pipeline_end = datetime.now(KST)
        elapsed = (pipeline_end - pipeline_start).total_seconds()

        kst_target = prediction_target.replace(tzinfo=timezone.utc).astimezone(KST)

        result = {
            'success': True,
            'prediction_date': kst_target.strftime('%Y-%m-%d %H:%M'),
            'prediction': prediction,
            'prediction_label': 'UP' if prediction == 1 else 'DOWN',
            'confidence': float(confidence),
            'predicted_accuracy_pct': model_details.get('predicted_accuracy_pct', 0),
            'model_details': model_details,
            'training_performed': training_performed,
            'current_price_usd': current_usd,
            'current_price_krw': current_krw,
        }

        print(f'\n\n{"#"*60}')
        print(f'ğŸ”® v7E PREDICTION RESULT')
        print(f'{"#"*60}')
        print(f'')
        print(f'  ì˜ˆì¸¡ ëŒ€ìƒ ì‹œì : {kst_target.strftime("%Y-%m-%d %H:%M")} KST')
        print(f'  ì˜ˆì¸¡ ê²°ê³¼: {"ğŸŸ¢ UP (ìƒìŠ¹)" if prediction == 1 else "ğŸ”´ DOWN (í•˜ë½)"}')
        if current_usd:
            print(f'  í˜„ì¬ ê°€ê²©: ${current_usd:,.2f}')
        if current_krw:
            print(f'  í˜„ì¬ ê°€ê²©: â‚©{current_krw:,.0f}')
        print(f'  ì‹ ë¢°ë„: {confidence*100:.1f}%')
        print(f'  ì˜ˆìƒ ì •í™•ë„: {model_details.get("predicted_accuracy_pct", 0):.1f}%')
        print(f'  Regime: {model_details.get("regime", "unknown")}')
        print(f'')
        print(f'  ê°œë³„ ëª¨ë¸:')
        for name, prob in model_details.get('individual_predictions', {}).items():
            print(f'    {name}: {prob:.4f} ({"UP" if prob > 0.5 else "DOWN"})')
        print(f'  Meta-Learner Stacking: {model_details.get("meta_stacking_probability", 0):.4f}')
        print(f'  Regime Dynamic: {model_details.get("regime_probability", 0):.4f}')
        print(f'  Final: {model_details.get("final_probability", 0):.4f}')
        print(f'')
        print(f'  ì¦ë¶„ í•™ìŠµ: {"ìˆ˜í–‰ë¨" if training_performed else "ìƒëµ"}')
        print(f'  ì†Œìš” ì‹œê°„: {elapsed:.1f}ì´ˆ')
        print(f'{"#"*60}')

        return result

    except Exception as e:
        log(f'âŒ íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {e}', important=True)
        import traceback
        traceback.print_exc()
        return {'success': False, 'error': str(e)}

log('âœ… ë©”ì¸ íŒŒì´í”„ë¼ì¸ ì •ì˜ ì™„ë£Œ')

# ## â–¶ï¸ 8. ì‹¤í–‰!

# ==========================================
# ì‹¤í–‰!
# ==========================================
result = run_daily_pipeline()


