# # ğŸš€ Complete BTC Data Pipeline for Colab
# 
# ì´ ë…¸íŠ¸ë¶ì€ ë¹„íŠ¸ì½”ì¸ ê°€ê²© ì˜ˆì¸¡ì„ ìœ„í•œ ì™„ì „í•œ ë°ì´í„° íŒŒì´í”„ë¼ì¸ì…ë‹ˆë‹¤.
# 
# **ì£¼ìš” ê¸°ëŠ¥:**
# 1. ë‰´ìŠ¤ ìˆ˜ì§‘ + GPT ê°ì„± ë¶„ì„ â†’ `raw_sentiment` í…Œì´ë¸” ì €ì¥
# 2. ê°€ê²©/ì§€í‘œ í¬ë¡¤ë§ (BTC, FRED, Yahoo, ì˜¨ì²´ì¸ ë“±)
# 3. í”¼ì³ ì—”ì§€ë‹ˆì–´ë§ (ê¸°ìˆ ì  ì§€í‘œ, ê°ì„± íŒŒìƒ í”¼ì³ ë“±)
# 4. `features_master` í…Œì´ë¸”ì— ì €ì¥
# 
# **ëª¨ë“œ:**
# - `INCREMENTAL` (ê¸°ë³¸ê°’): DBì˜ ë§ˆì§€ë§‰ ë‚ ì§œ ì´í›„ë§Œ ìˆ˜ì§‘
# - `FULL`: 2017-01-01ë¶€í„° ì „ì²´ ìˆ˜ì§‘

# ## ğŸ“¦ 1. í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜

# Google Drive ë§ˆìš´íŠ¸
import sys, os
_IS_COLAB_EARLY = 'google.colab' in sys.modules or ('google' in sys.modules and hasattr(sys.modules.get('google', None), 'colab'))
if not _IS_COLAB_EARLY:
    try:
        import importlib.util
        _IS_COLAB_EARLY = importlib.util.find_spec('google.colab') is not None and 'COLAB_RELEASE_TAG' in os.environ
    except Exception:
        pass

if _IS_COLAB_EARLY:
    from google.colab import drive
    drive.mount('/content/drive')
    print("âœ… Google Drive ë§ˆìš´íŠ¸ ì™„ë£Œ")

# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜

print("âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ì™„ë£Œ")

# ## ğŸ”§ 2. í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ë° í™˜ê²½ë³€ìˆ˜ ë¡œë“œ

import pandas as pd
import numpy as np
import logging
import json
import re
import time
from datetime import datetime, timedelta, timezone
from dotenv import load_dotenv
from tqdm import tqdm

# í¬ë¡¤ë§/ì§€í‘œ ë¼ì´ë¸ŒëŸ¬ë¦¬
import ccxt
import yfinance as yf
from fredapi import Fred
import requests
from ta.trend import SMAIndicator, ADXIndicator, CCIIndicator, MACD
from ta.momentum import RSIIndicator, WilliamsRIndicator
from ta.volatility import BollingerBands, AverageTrueRange

# ê°ì„± ë¶„ì„ìš© ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from pygooglenews import GoogleNews

# Supabase í´ë¼ì´ì–¸íŠ¸
from supabase import create_client

print("âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ")

# ==============================================================================
# ğŸ” 1. ì–´ë–¤ í™˜ê²½ì—ì„œë“  ì•Œì•„ì„œ í‚¤ë¥¼ ì°¾ì•„ì˜¤ëŠ” í•˜ì´ë¸Œë¦¬ë“œ ë¡œë“œ êµ¬ì„±
# ==============================================================================
import sys
import os

IS_COLAB = 'google.colab' in sys.modules
IS_KAGGLE = 'kaggle_secrets' in sys.modules or os.path.exists('/kaggle/working')
IS_GITHUB_ACTIONS = os.getenv('GITHUB_ACTIONS') == 'true'

# [ë¡œì»¬ & ê¸°ì¡´ Colab ë“œë¼ì´ë¸Œ ì‚¬ìš©ììš©] .env íŒŒì¼ ë¡œë“œ
try:
    from dotenv import load_dotenv
    for env_path in ['/content/drive/MyDrive/2526Winter_Sideproject/.env', '.env', '/content/.env']:
        if os.path.exists(env_path):
            load_dotenv(env_path)
            print(f"ğŸ”§ .env íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {env_path}")
            break
except ImportError:
    pass

# [Colab Secrets / Kaggle Secrets ì „ìš© ì‚¬ìš©ììš©]
if IS_COLAB:
    from google.colab import drive, userdata
    drive.mount('/content/drive')
    os.environ['PROJECT_ROOT'] = '/content/drive/MyDrive/2526Winter_Sideproject'
    for key in ['SUPABASE_URL', 'SUPABASE_KEY', 'SUPABASE_SERVICE_KEY', 'OPENAI_API_KEY', 'FRED_API_KEY']:
        try:
            val = userdata.get(key)
            if val: os.environ[key] = val
        except: pass
elif IS_KAGGLE:
    from kaggle_secrets import UserSecretsClient
    user_secrets = UserSecretsClient()
    os.environ['PROJECT_ROOT'] = '/kaggle/working'
    for key in ['SUPABASE_URL', 'SUPABASE_KEY', 'SUPABASE_SERVICE_KEY', 'OPENAI_API_KEY', 'FRED_API_KEY']:
        try:
            val = user_secrets.get_secret(key)
            if val: os.environ[key] = val
        except: pass
elif IS_GITHUB_ACTIONS:
    os.environ['PROJECT_ROOT'] = os.getenv('GITHUB_WORKSPACE', os.getcwd())
else:
    os.environ['PROJECT_ROOT'] = os.getcwd()

# ==============================================================================
# ğŸš€ 2. ë³€ìˆ˜ í• ë‹¹ ë° ì—°ê²°
# ==============================================================================
PROJECT_ROOT = os.getenv("PROJECT_ROOT", os.getcwd())
SUPABASE_URL = os.getenv("SUPABASE_URL")
SERVICE_KEY = os.getenv("SUPABASE_SERVICE_KEY", os.getenv("SUPABASE_KEY")) 
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
FRED_API_KEY = os.getenv("FRED_API_KEY")

if not SUPABASE_URL or not SERVICE_KEY:
    raise ValueError("âŒ ì¹˜ëª…ì  ì˜¤ë¥˜: SUPABASE_URL ë˜ëŠ” SUPABASE_SERVICE_KEYë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")

from supabase import create_client
supabase = create_client(SUPABASE_URL, SERVICE_KEY)

print("âœ… Supabase ë§ˆìŠ¤í„° ê¶Œí•œ(Service Role) ì—°ê²° ì„±ê³µ ğŸ”“")

import logging
# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s | %(levelname)s | %(message)s')
logger = logging.getLogger("CompletePipeline")
print("âœ… í™˜ê²½ ì„¤ì • ì™„ë£Œ")

# ## ğŸ¯ 3. ê°ì„± ë¶„ì„ ê´€ë ¨ í•¨ìˆ˜
# 
# Google Newsì—ì„œ ë¹„íŠ¸ì½”ì¸ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•˜ê³ , GPT-4o-minië¥¼ ì‚¬ìš©í•˜ì—¬ ê°ì„± ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

def init_sentiment_analyzer():
    """ê°ì„± ë¶„ì„ê¸° ì´ˆê¸°í™”"""
    if not OPENAI_API_KEY:
        logger.warning("ê°ì„± ë¶„ì„ ë¹„í™œì„±í™”: OPENAI_API_KEY ì—†ìŒ")
        return None, None
    
    try:
        llm = ChatOpenAI(model="gpt-4o-mini", temperature=0, openai_api_key=OPENAI_API_KEY)
        gn = GoogleNews(lang='en', country='US')
        
        sentiment_prompt = PromptTemplate(
            input_variables=["headlines_text"],
            template="""
            You are a professional crypto market analyst.
            Below are the top 5 Bitcoin news headlines for a specific day.

            HEADLINES:
            {headlines_text}

            INSTRUCTIONS:
            1. Analyze all headlines collectively to determine the overall market sentiment.
            2. Return ONLY a JSON object with no markdown formatting.
            3. Keys required:
               - "sentiment": float between -1.0 (Negative) to 1.0 (Positive)
               - "impact": float between 0.0 (Low) to 1.0 (High)

            JSON OUTPUT EXAMPLE:
            {{"sentiment": 0.45, "impact": 0.8}}
            """
        )
        sentiment_chain = sentiment_prompt | llm
        logger.info("ê°ì„± ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ (GPT-4o-mini)")
        return sentiment_chain, gn
    except Exception as e:
        logger.error(f"ê°ì„± ë¶„ì„ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return None, None


def parse_llm_response(response_content):
    """LLM ì‘ë‹µ íŒŒì‹±"""
    content = response_content.replace("```json", "").replace("```", "").strip()
    try:
        data = json.loads(content)
        return float(data.get('sentiment', 0.0)), float(data.get('impact', 0.0)), None
    except json.JSONDecodeError:
        try:
            match = re.search(r'\{.*"sentiment".*\}', content, re.DOTALL)
            if match:
                data = json.loads(match.group(0))
                return float(data.get('sentiment', 0.0)), float(data.get('impact', 0.0)), None
        except:
            pass
        return 0.0, 0.0, f"Parse Error: {content[:50]}..."


def fetch_and_analyze_daily(target_date, sentiment_chain, gn):
    """íŠ¹ì • ë‚ ì§œì˜ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•˜ê³  GPTë¡œ ë¶„ì„"""
    next_day = (datetime.strptime(target_date, "%Y-%m-%d") + timedelta(days=1)).strftime("%Y-%m-%d")

    # 1. êµ¬ê¸€ ë‰´ìŠ¤ ê²€ìƒ‰
    try:
        search = gn.search(f'Bitcoin OR BTC after:{target_date} before:{next_day}')
        headlines = [entry.title for entry in search['entries'][:5]]
    except Exception as e:
        return None, f"Search Fail: {e}"

    if not headlines:
        return {
            "date": target_date,
            "sentiment_score": 0.0,
            "impact_score": 0.0,
            "headline_summary": "No News",
            "error_log": None
        }, None

    # 2. OpenAI ë¶„ì„
    headlines_text = "\n".join([f"{i+1}. {h}" for i, h in enumerate(headlines)])
    try:
        res = sentiment_chain.invoke({"headlines_text": headlines_text})
        sentiment, impact, parse_err = parse_llm_response(res.content)

        result = {
            "date": target_date,
            "sentiment_score": sentiment,
            "impact_score": impact,
            "headline_summary": headlines[0],
            "error_log": parse_err
        }
        return result, None

    except Exception as e:
        return {
            "date": target_date,
            "sentiment_score": 0.0,
            "impact_score": 0.0,
            "headline_summary": headlines[0] if headlines else "",
            "error_log": str(e)
        }, f"Analysis Fail: {e}"


def collect_and_save_sentiment(start_date, end_date):
    """
    [Step 1A] ë‰´ìŠ¤ ìˆ˜ì§‘ + GPT ê°ì„± ë¶„ì„ â†’ raw_sentiment ì €ì¥
    """
    logger.info("=" * 80)
    logger.info("Step 1A: ë‰´ìŠ¤ ìˆ˜ì§‘ ë° GPT ê°ì„± ë¶„ì„")
    logger.info("=" * 80)
    
    sentiment_chain, gn = init_sentiment_analyzer()
    if sentiment_chain is None:
        logger.warning("ê°ì„± ë¶„ì„ ìŠ¤í‚µ (ë¹„í™œì„±í™”ë¨)")
        return
    
    # raw_sentimentì—ì„œ ë§ˆì§€ë§‰ ë‚ ì§œ í™•ì¸
    try:
        response = supabase.table("raw_sentiment") \
            .select("date").order("date", desc=True).limit(1).execute()
        
        if response.data:
            last_date_str = response.data[0]['date']
            sentiment_start = datetime.strptime(last_date_str, "%Y-%m-%d").replace(tzinfo=timezone.utc) + timedelta(days=1)
            logger.info(f"raw_sentiment ë§ˆì§€ë§‰ ë‚ ì§œ: {last_date_str}")
        else:
            sentiment_start = start_date
    except Exception as e:
        logger.warning(f"raw_sentiment ì¡°íšŒ ì‹¤íŒ¨: {e}")
        sentiment_start = start_date
    
    # ì–´ì œê¹Œì§€ë§Œ ìˆ˜ì§‘ (ì˜¤ëŠ˜ ë‰´ìŠ¤ëŠ” ì•„ì§ ì™„ì „í•˜ì§€ ì•ŠìŒ)
    sentiment_end = min(end_date, datetime.now(timezone.utc) - timedelta(days=1))
    
    total_days = (sentiment_end - sentiment_start).days + 1
    if total_days <= 0:
        logger.info("ê°ì„± ë°ì´í„°ê°€ ì´ë¯¸ ìµœì‹ ì…ë‹ˆë‹¤.")
        return
    
    logger.info(f"ê°ì„± ë¶„ì„ ëŒ€ìƒ: {total_days}ì¼ ({sentiment_start.strftime('%Y-%m-%d')} ~ {sentiment_end.strftime('%Y-%m-%d')})")
    
    curr = sentiment_start
    records_buffer = []
    BATCH_SIZE = 10
    
    pbar = tqdm(total=total_days, desc="ë‰´ìŠ¤ ê°ì„± ë¶„ì„")
    
    while curr <= sentiment_end:
        target_date_str = curr.strftime("%Y-%m-%d")
        
        result, err = fetch_and_analyze_daily(target_date_str, sentiment_chain, gn)
        
        if err:
            logger.warning(f"{target_date_str} ì—ëŸ¬: {err}")
        
        if result:
            records_buffer.append(result)
        
        # ë°°ì¹˜ ì €ì¥
        if len(records_buffer) >= BATCH_SIZE:
            try:
                supabase.table("raw_sentiment").upsert(records_buffer, on_conflict="date").execute()
                logger.info(f"{len(records_buffer)}ê°œ ê°ì„± ë°ì´í„° ì €ì¥ (~{target_date_str})")
                records_buffer = []
            except Exception as e:
                logger.error(f"raw_sentiment ì €ì¥ ì‹¤íŒ¨: {e}")
        
        curr += timedelta(days=1)
        pbar.update(1)
        time.sleep(0.1)
    
    # ë‚¨ì€ ë²„í¼ ì €ì¥ (ë£¨í”„ ì¢…ë£Œ í›„ ì”ì—¬ ë°ì´í„°)
    if records_buffer:
        try:
            supabase.table("raw_sentiment").upsert(records_buffer, on_conflict="date").execute()
            logger.info(f"{len(records_buffer)}ê°œ ê°ì„± ë°ì´í„° ì €ì¥ (ì”ì—¬)")
        except Exception as e:
            logger.error(f"raw_sentiment ì”ì—¬ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    pbar.close()
    logger.info("ë‰´ìŠ¤ ê°ì„± ë¶„ì„ ì™„ë£Œ!")

print("âœ… ê°ì„± ë¶„ì„ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ")

# ## ğŸ”§ 4. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# 
# DB ì¡°íšŒ ë° ë°ì´í„° ë¡œë“œ ê´€ë ¨ í—¬í¼ í•¨ìˆ˜ë“¤ì…ë‹ˆë‹¤.

def get_latest_date_from_db():
    """features_masterì—ì„œ ìµœì‹  ë‚ ì§œ ì¡°íšŒ"""
    try:
        response = supabase.table("features_master") \
            .select("date").order("date", desc=True).limit(1).execute()
        if not response.data:
            return None
        return pd.to_datetime(response.data[0]['date']).replace(tzinfo=timezone.utc)
    except Exception as e:
        logger.error(f"DB ë‚ ì§œ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return None


def fetch_all_raw_sentiment():
    """raw_sentiment ì „ì²´ ë°ì´í„° ë¡œë“œ"""
    logger.info("raw_sentiment ì „ì²´ ë°ì´í„° ë¡œë“œ ì¤‘...")
    all_data = []
    page_size = 1000
    offset = 0
    
    while True:
        response = supabase.table("raw_sentiment") \
            .select("date, sentiment_score, impact_score") \
            .order("date") \
            .range(offset, offset + page_size - 1) \
            .execute()
        
        if not response.data:
            break
        
        all_data.extend(response.data)
        offset += page_size
        
        if len(response.data) < page_size:
            break
    
    if not all_data:
        return pd.DataFrame()
    
    df = pd.DataFrame(all_data)
    df['date'] = pd.to_datetime(df['date'])
    df.set_index('date', inplace=True)
    df.sort_index(inplace=True)
    logger.info(f"  raw_sentiment: {len(df)}ê°œ ë¡œë“œ")
    return df

print("âœ… ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ")

# ## ğŸ“Š 5. ë°ì´í„° í¬ë¡¤ë§ í•¨ìˆ˜ë“¤
# 
# BTC ê°€ê²©, FRED ê²½ì œ ì§€í‘œ, Yahoo Finance ì‹œì¥ ì§€í‘œ, ì˜¨ì²´ì¸ ë°ì´í„° ë“±ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.

def crawl_btc_price(start_dt, end_dt=None):
    """Coinbase BTC ê°€ê²© ìˆ˜ì§‘ (ì‹œê°„ë´‰)"""
    logger.info(f"BTC ê°€ê²© ìˆ˜ì§‘: {start_dt.date()}")
    try:
        exchange = ccxt.coinbase()
        limit = 300
        since = int(start_dt.timestamp() * 1000)
        end_ts = int(end_dt.timestamp() * 1000) if end_dt else int(datetime.now(timezone.utc).timestamp() * 1000)
        all_ohlcv = []
        
        max_retries = 5
        retry_count = 0
        
        while since < end_ts:
            try:
                ohlcv = exchange.fetch_ohlcv('BTC/USD', '1h', since, limit=limit)
                if not ohlcv:
                    break
                all_ohlcv += ohlcv
                last_ts = ohlcv[-1][0]
                since = last_ts + 3600000
                retry_count = 0  # ì„±ê³µ ì‹œ ë¦¬ì…‹
                if since >= end_ts:
                    break
                time.sleep(0.1)
            except Exception as e:
                retry_count += 1
                logger.warning(f"BTC API Retry ({retry_count}/{max_retries}): {e}")
                if retry_count >= max_retries:
                    logger.error("BTC API ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼, ìˆ˜ì§‘ëœ ë°ì´í„°ë¡œ ì§„í–‰")
                    break
                time.sleep(2 ** retry_count)  # exponential backoff
                continue
        
        if not all_ohlcv:
            return pd.DataFrame()
        
        df = pd.DataFrame(all_ohlcv, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])
        df['date'] = pd.to_datetime(df['timestamp'], unit='ms')
        df.set_index('date', inplace=True)
        df.drop(columns=['timestamp'], inplace=True)
        logger.info(f"  BTC ê°€ê²©: {len(df)} rows")
        return df[~df.index.duplicated(keep='first')].sort_index()
    except Exception as e:
        logger.error(f"BTC ê°€ê²© ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return pd.DataFrame()


def crawl_fred_indicators(start_date):
    """FRED ê²½ì œ ì§€í‘œ ìˆ˜ì§‘"""
    logger.info(f"FRED ì§€í‘œ ìˆ˜ì§‘: {start_date.date()}")
    fred_codes = {
        'WALCL': 'Fed_Balance_Sheet', 'WTREGEN': 'TGA', 'RRPONTSYD': 'Reverse_Repo',
        'FEDFUNDS': 'Base_Rate', 'CPIAUCSL': 'CPI', 'UNRATE': 'Unemployment',
        'M2SL': 'M2_Liquidity', 'DGS10': 'US_10Y_Yield', 'DGS2': 'US_2Y_Yield'
    }
    try:
        fred = Fred(api_key=FRED_API_KEY)
        start_str = start_date.strftime('%Y-%m-%d')
        macro_data = {}
        
        for code, name in fred_codes.items():
            try:
                macro_data[name] = fred.get_series(code, observation_start=start_str)
            except:
                pass
        
        df = pd.DataFrame(macro_data)
        if not df.empty:
            df.index = pd.to_datetime(df.index)
            df = df.resample('D').ffill().ffill().bfill()
            
            if all(c in df.columns for c in ['Fed_Balance_Sheet', 'TGA', 'Reverse_Repo']):
                df['net_liquidity'] = df['Fed_Balance_Sheet'] - df['TGA'] - df['Reverse_Repo']
                df['liquidity_lag_7'] = df['net_liquidity'].shift(7)
                df['liquidity_lag_30'] = df['net_liquidity'].shift(30)
        
        logger.info(f"  FRED: {len(df)} rows")
        return df
    except Exception as e:
        logger.error(f"FRED ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return pd.DataFrame()


def crawl_market_indicators(start_date):
    """Yahoo Finance ì‹œì¥ ì§€í‘œ ìˆ˜ì§‘"""
    logger.info(f"ì‹œì¥ ì§€í‘œ ìˆ˜ì§‘: {start_date.date()}")
    tickers = {
        '^NDX': 'NASDAQ', 'DX-Y.NYB': 'DXY', 'GC=F': 'GOLD', 
        'CL=F': 'OIL', '^VIX': 'VIX', '^TNX': 'US10Y', 'ETH-USD': 'ETH'
    }
    try:
        start_str = start_date.strftime('%Y-%m-%d')
        df = yf.download(list(tickers.keys()), start=start_str, progress=False, auto_adjust=True)['Close']
        
        if isinstance(df.columns, pd.MultiIndex):
            df.columns = df.columns.get_level_values(0)
        df = df.rename(columns=tickers)
        if df.index.tz is not None:
            df.index = df.index.tz_localize(None)
        
        df = df.resample('D').ffill()
        logger.info(f"  ì‹œì¥ ì§€í‘œ: {len(df)} rows")
        return df
    except Exception as e:
        logger.error(f"ì‹œì¥ ì§€í‘œ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return pd.DataFrame()


def crawl_exchange_rate(start_date):
    """í™˜ìœ¨ ë°ì´í„° ìˆ˜ì§‘"""
    try:
        start_str = start_date.strftime('%Y-%m-%d')
        df = yf.download('KRW=X', start=start_str, progress=False, auto_adjust=True)
        if isinstance(df.columns, pd.MultiIndex):
            df.columns = df.columns.get_level_values(0)
        df = df.rename(columns={'Close': 'exchange_rate'})[['exchange_rate']]
        if df.index.tz is not None:
            df.index = df.index.tz_localize(None)
        df = df.resample('D').ffill()
        return df
    except Exception as e:
        logger.warning(f"í™˜ìœ¨ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return pd.DataFrame()


def crawl_upbit_price(start_date):
    """ì—…ë¹„íŠ¸ BTC/KRW ê°€ê²© ìˆ˜ì§‘"""
    try:
        import pyupbit
        diff = datetime.now(timezone.utc).replace(tzinfo=None) - start_date.replace(tzinfo=None)
        count_hours = int(diff.total_seconds() / 3600) + 24
        
        df = pyupbit.get_ohlcv('KRW-BTC', interval='minute60', count=min(count_hours, 200))
        if df is not None and not df.empty:
            df = df.rename(columns={'close': 'close_upbit'})
            # timezone ì•ˆì „ ì²˜ë¦¬: ì´ë¯¸ tz-awareì¼ ìˆ˜ ìˆìŒ
            if df.index.tz is not None:
                df.index = df.index.tz_convert('UTC').tz_localize(None)
            else:
                df.index = df.index.tz_localize('Asia/Seoul').tz_convert('UTC').tz_localize(None)
            return df[['close_upbit']]
        return pd.DataFrame()
    except Exception as e:
        logger.warning(f"ì—…ë¹„íŠ¸ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return pd.DataFrame()


def crawl_blockchain_data(chart_name):
    """Blockchain.info ì˜¨ì²´ì¸ ë°ì´í„° ìˆ˜ì§‘"""
    try:
        url = f"https://api.blockchain.info/charts/{chart_name}?timespan=all&format=json&sampled=true"
        resp = requests.get(url, timeout=30).json()
        df = pd.DataFrame(resp['values'])
        df['x'] = pd.to_datetime(df['x'], unit='s')
        df.set_index('x', inplace=True)
        col_name = chart_name.replace('-', '_')
        df = df.rename(columns={'y': col_name})
        return df.resample('h').ffill()
    except Exception as e:
        logger.warning(f"ì˜¨ì²´ì¸ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨ ({chart_name}): {e}")
        return pd.DataFrame()


def crawl_onchain_data():
    """ì˜¨ì²´ì¸ ì§€í‘œ ìˆ˜ì§‘"""
    logger.info("ì˜¨ì²´ì¸ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
    df_hash = crawl_blockchain_data('hash-rate')
    df_addr = crawl_blockchain_data('n-unique-addresses')
    return df_hash, df_addr


def crawl_fear_greed_index():
    """Fear & Greed Index ìˆ˜ì§‘"""
    logger.info("Fear & Greed Index ìˆ˜ì§‘ ì¤‘...")
    try:
        url = "https://api.alternative.me/fng/?limit=0"
        resp = requests.get(url, timeout=30).json()
        fng = pd.DataFrame(resp['data'])
        fng['date'] = pd.to_datetime(fng['timestamp'].astype(int), unit='s')
        fng['fng_value'] = fng['value'].astype(int)
        fng = fng.set_index('date')[['fng_value']]
        fng = fng.resample('h').ffill()
        logger.info(f"  F&G Index: {len(fng)} rows")
        return fng
    except Exception as e:
        logger.error(f"F&G Index ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return pd.DataFrame()


def collect_all_data(start_date, end_date):
    """
    [Step 1B] ëª¨ë“  ë°ì´í„° ì†ŒìŠ¤ì—ì„œ ìˆ˜ì§‘í•˜ì—¬ ë³‘í•©
    """
    logger.info("=" * 80)
    logger.info("Step 1B: ê°€ê²©/ì§€í‘œ ë°ì´í„° ìˆ˜ì§‘")
    logger.info("=" * 80)
    
    # BTC ê°€ê²© (ê¸°ì¤€ ë°ì´í„°í”„ë ˆì„)
    df_btc = crawl_btc_price(start_date, end_date)
    if df_btc.empty:
        logger.error("BTC ê°€ê²© ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
        return pd.DataFrame()
    
    # ë‹¤ë¥¸ ë°ì´í„° ìˆ˜ì§‘
    df_fred = crawl_fred_indicators(start_date)
    df_market = crawl_market_indicators(start_date)
    df_fx = crawl_exchange_rate(start_date)
    df_upbit = crawl_upbit_price(start_date)
    df_hash, df_addr = crawl_onchain_data()
    df_fng = crawl_fear_greed_index()
    
    # BTC ë°ì´í„°í”„ë ˆì„ì— ë³‘í•©
    df = df_btc.copy()
    
    # ì¼ë³„ ë°ì´í„° â†’ ì‹œê°„ë³„ë¡œ í™•ì¥ (forward fill)
    for source_df, name in [(df_fred, 'FRED'), (df_market, 'Market'), (df_fx, 'FX')]:
        if not source_df.empty:
            source_df = source_df.resample('h').ffill()
            for col in source_df.columns:
                if col not in df.columns:
                    df = df.join(source_df[[col]], how='left')
    
    # ì‹œê°„ë³„ ë°ì´í„° ì§ì ‘ ë³‘í•©
    for source_df, name in [(df_upbit, 'Upbit'), (df_hash, 'Hash'), (df_addr, 'Addr'), (df_fng, 'FnG')]:
        if not source_df.empty:
            for col in source_df.columns:
                if col not in df.columns:
                    df = df.join(source_df[[col]], how='left')
    
    # Forward fill
    df = df.ffill().bfill()
    
    logger.info(f"ë°ì´í„° ë³‘í•© ì™„ë£Œ: {len(df)} rows, {len(df.columns)} columns")
    return df

print("âœ… í¬ë¡¤ë§ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ")

# ## ğŸ”¨ 6. í”¼ì³ ì—”ì§€ë‹ˆì–´ë§ - ê¸°ìˆ ì  ì§€í‘œ
# 
# ì´ë™í‰ê· , ë³¼ë¦°ì € ë°´ë“œ, RSI, MACD ë“± ê¸°ìˆ ì  ì§€í‘œë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

def calculate_technical_indicators(df):
    """ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°"""
    df = df.copy()
    if 'close' not in df.columns:
        return df
    
    close = df['close']
    high = df['high'] if 'high' in df.columns else close
    low = df['low'] if 'low' in df.columns else close
    volume = df['volume'] if 'volume' in df.columns else pd.Series(0, index=df.index)
    
    # ì´ë™í‰ê· 
    df['SMA_20'] = SMAIndicator(close, window=20).sma_indicator()
    df['SMA_60'] = SMAIndicator(close, window=60).sma_indicator()
    df['golden_cross'] = (df['SMA_20'] > df['SMA_60']).astype(int)
    df['Dist_SMA_60'] = (close - df['SMA_60']) / df['SMA_60']
    
    # ë³¼ë¦°ì € ë°´ë“œ
    bb = BollingerBands(close, window=20, window_dev=2)
    df['BB_upper'] = bb.bollinger_hband()
    df['BB_lower'] = bb.bollinger_lband()
    df['BB_width'] = bb.bollinger_wband()
    df['BB_position'] = bb.bollinger_pband()
    
    # ëª¨ë©˜í…€ ì§€í‘œ
    df['RSI'] = RSIIndicator(close, window=14).rsi()
    df['RSI_14'] = df['RSI']
    df['rsi_14'] = df['RSI']
    macd = MACD(close)
    df['MACD'] = macd.macd()
    df['MACD_signal'] = macd.macd_signal()
    df['MACD_hist'] = macd.macd_diff()
    df['CCI'] = CCIIndicator(high, low, close, window=20).cci()
    df['WillR'] = WilliamsRIndicator(high, low, close, lbp=14).williams_r()
    df['log_ret_1'] = np.log(close / close.shift(1))
    
    # ë³€ë™ì„± ì§€í‘œ
    df['ATR'] = AverageTrueRange(high, low, close, window=14).average_true_range()
    df['ATR_ratio'] = df['ATR'] / close
    df['volatility_30'] = close.pct_change().rolling(window=30*24).std()
    hv = close.pct_change().rolling(window=24*30).std()
    df['HV_Rank'] = hv.rolling(window=24*365, min_periods=24*30).rank(pct=True)
    
    # ì¶”ì„¸ ì§€í‘œ
    df['ADX'] = ADXIndicator(high, low, close, window=14).adx()
    df['Trend_Strength'] = df['ADX'] * np.where(df['MACD'] > 0, 1, -1)
    
    # ê±°ë˜ëŸ‰ ì´ìƒ íƒì§€
    vol_ma = volume.rolling(24*7).mean()
    df['Vol_Anomaly'] = (volume > vol_ma * 2).astype(int)
    
    # ë³µí•© ì§€í‘œ
    df['rsi_x_macd'] = df['RSI'] * df['MACD']
    df['rsi_x_bb'] = df['RSI'] * df['BB_position']
    df['price_x_vol'] = close.pct_change() * volume.pct_change()
    
    return df

print("âœ… ê¸°ìˆ ì  ì§€í‘œ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ")

# ## ğŸ”¨ 7. í”¼ì³ ì—”ì§€ë‹ˆì–´ë§ - ì˜¨ì²´ì¸ í”„ë¡ì‹œ
# 
# MVRV Z-Score, Puell Multiple, NUPL, SOPR ë“± ì˜¨ì²´ì¸ ì§€í‘œë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

def calculate_onchain_proxies(df):
    """ì˜¨ì²´ì¸ í”„ë¡ì‹œ ì§€í‘œ ê³„ì‚°"""
    df = df.copy()
    if 'close' not in df.columns:
        return df
    
    close = df['close']
    rv_proxy = close.rolling(window=4800, min_periods=1).mean()
    rv_std = close.rolling(window=4800, min_periods=1).std()
    df['MVRV_Z_Score'] = ((close - rv_proxy) / rv_std).fillna(0)
    ma_365 = close.rolling(window=8760, min_periods=1).mean()
    df['Puell_Multiple'] = (close / ma_365).fillna(1)
    df['NUPL'] = ((close - rv_proxy) / close).fillna(0)
    df['SOPR'] = (close / close.shift(24)).fillna(1)
    
    try:
        rsi_long = RSIIndicator(close=close, window=2000).rsi()
        df['Reserve_Risk'] = (close / (rv_proxy * (100 - rsi_long + 1) + 1e-9)).fillna(0)
    except:
        df['Reserve_Risk'] = 0
    
    if 'hash_rate' in df.columns and 'close' in df.columns:
        hash_norm = df['hash_rate'].pct_change(24*7)
        price_norm = df['close'].pct_change(24*7)
        df['Hash_Price_Div'] = hash_norm - price_norm
    else:
        df['Hash_Price_Div'] = 0
    
    # active_addresses ë§¤í•‘ (n_unique_addressesì™€ ë™ì¼)
    if 'n_unique_addresses' in df.columns:
        df['active_addresses'] = df['n_unique_addresses']
    else:
        df['active_addresses'] = 0
    
    return df

print("âœ… ì˜¨ì²´ì¸ í”„ë¡ì‹œ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ")

# ## ğŸ”¨ 8. í”¼ì³ ì—”ì§€ë‹ˆì–´ë§ - ì‚¬ì´í´ ë° ì‹œê°„ í”¼ì³
# 
# ë°˜ê°ê¸° ì‚¬ì´í´, ì‹œê°„ ì •ë³´, ê±°ë˜ ì„¸ì…˜ ë“±ì˜ í”¼ì³ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

def calculate_cycle_features(df):
    """ë°˜ê°ê¸° ì‚¬ì´í´ ë° ì‹œê°„ í”¼ì³ ê³„ì‚°"""
    df = df.copy()
    halvings = [
        pd.to_datetime('2012-11-28'), pd.to_datetime('2016-07-09'),
        pd.to_datetime('2020-05-11'), pd.to_datetime('2024-04-20')
    ]
    winter_start = pd.to_datetime('2022-01-01')
    
    def get_cycle_info(dt):
        past_halvings = [h for h in halvings if h <= dt]
        if past_halvings:
            last_halving = past_halvings[-1]
            days_since = (dt - last_halving).days
            cycle_idx = len(past_halvings)
        else:
            days_since = 0
            cycle_idx = 0
        cycle_progress = days_since / 1460 if days_since > 0 else 0
        days_since_winter = (dt - winter_start).days if dt >= winter_start else 0
        return days_since, cycle_progress, cycle_idx, days_since_winter
    
    results = df.index.to_series().apply(get_cycle_info)
    df['days_since_halving'] = [r[0] for r in results]
    df['cycle_progress'] = [r[1] for r in results]
    df['halving_cycle_index'] = [r[2] for r in results]
    df['days_since_winter'] = [r[3] for r in results]
    
    # ì‹œê°„ í”¼ì³
    df['hour'] = df.index.hour
    df['dayofweek'] = df.index.dayofweek
    df['is_weekend'] = df.index.dayofweek.isin([5, 6]).astype(int)
    df['day_cos'] = np.cos(2 * np.pi * df['dayofweek'] / 7)
    hour = df.index.hour
    df['is_ny_open'] = ((hour >= 13) & (hour <= 14)).astype(int)
    df['is_london_close'] = ((hour >= 15) & (hour <= 16)).astype(int)
    df['session_overlap'] = ((hour >= 13) & (hour <= 16)).astype(int)
    df['is_us_market_open'] = ((hour >= 13) & (hour <= 20) & (df['is_weekend'] == 0)).astype(int)
    
    return df

print("âœ… ì‚¬ì´í´/ì‹œê°„ í”¼ì³ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ")

# ## ğŸ”¨ 9. í”¼ì³ ì—”ì§€ë‹ˆì–´ë§ - ë§¤í¬ë¡œ ê²½ì œ íŒŒìƒ í”¼ì³
# 
# DXY, ê¸ˆë¦¬, VIX, ì›ìœ , ê¸ˆ ë“± ë§¤í¬ë¡œ ê²½ì œ ì§€í‘œ ê¸°ë°˜ íŒŒìƒ í”¼ì³ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

def calculate_macro_derived_features(df):
    """ë§¤í¬ë¡œ ê²½ì œ íŒŒìƒ ì§€í‘œ ê³„ì‚°"""
    df = df.copy()
    
    if 'DXY' in df.columns:
        df['dxy_ma_deviation'] = (df['DXY'] - df['DXY'].rolling(24*20).mean())
        df['dxy_change_7d_avg'] = df['DXY'].pct_change(24*7).rolling(24).mean()
        df['dxy_change_1d'] = df['DXY'].pct_change(24)
    
    if 'Base_Rate' in df.columns and 'CPI' in df.columns:
        # CPIëŠ” ì§€ìˆ˜(Index)ì´ë¯€ë¡œ ì „ë…„ ëŒ€ë¹„ ìƒìŠ¹ë¥ (Inflation Rate)ë¡œ ë³€í™˜
        inflation_rate = df['CPI'].pct_change(365 * 24) * 100
        df['real_interest_rate'] = df['Base_Rate'] - inflation_rate
        df['Real_Yield_Impact'] = (df['real_interest_rate'] > 0).astype(int)
    
    if 'VIX' in df.columns:
        vix_ma = df['VIX'].rolling(24*20).mean()
        df['vix_spike'] = (df['VIX'] > vix_ma * 1.5).astype(int)
    
    if 'OIL' in df.columns:
        df['oil_trend'] = df['OIL'].pct_change(24*7)
    
    if 'close' in df.columns and 'GOLD' in df.columns:
        df['btc_gold_ratio'] = df['close'] / df['GOLD']
    
    if 'NASDAQ' in df.columns and 'close' in df.columns:
        df['ndx_log_return_1d'] = np.log(df['NASDAQ'] / df['NASDAQ'].shift(1))
        df['ndx_log_return_7d'] = np.log(df['NASDAQ'] / df['NASDAQ'].shift(7))
        df['ndx_vol'] = df['NASDAQ'].pct_change().rolling(24*7).std()
        df['ndx_btc_corr'] = df['close'].rolling(24*30).corr(df['NASDAQ'])
        df['BTC_NDX_Weighted'] = df['ndx_btc_corr'] * df['ndx_log_return_1d']
    
    if 'M2_Liquidity' in df.columns:
        df['m2_change'] = df['M2_Liquidity'].pct_change(24*7)
    
    if 'US10Y' in df.columns:
        df['tnx_change'] = df['US10Y'].pct_change(24)
    
    if 'ETH' in df.columns and 'close' in df.columns:
        df['eth_btc_ratio'] = df['ETH'] / df['close']
        df['eth_btc_corr'] = df['close'].rolling(24*30).corr(df['ETH'])
    
    return df

print("âœ… ë§¤í¬ë¡œ íŒŒìƒ í”¼ì³ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ")

# ## ğŸ”¨ 10. í”¼ì³ ì—”ì§€ë‹ˆì–´ë§ - ê°€ê²©/ê±°ë˜ëŸ‰ íŒŒìƒ í”¼ì³
# 
# ê°€ê²© ë³€í™”ìœ¨, ê±°ë˜ëŸ‰ ë³€í™”ìœ¨, ê¹€ì¹˜ í”„ë¦¬ë¯¸ì—„ ë“±ì„ ê³„ì‚°í•©ë‹ˆë‹¤.

def calculate_price_derived_features(df):
    """ê°€ê²©/ê±°ë˜ëŸ‰ íŒŒìƒ ì§€í‘œ ê³„ì‚°"""
    df = df.copy()
    
    if 'close' in df.columns:
        df['price_change'] = df['close'].pct_change()
        df['return_lag_24'] = df['close'].pct_change(24)
        df['return_lag_48'] = df['close'].pct_change(48)
    
    if 'volume' in df.columns:
        df['vol_change'] = df['volume'].pct_change()
        df['vol_lag_24'] = df['volume'].pct_change(24)
        df['vol_lag_48'] = df['volume'].pct_change(48)
    
    if 'n_unique_addresses' in df.columns and 'close' in df.columns:
        price_trend = df['close'].pct_change(24*7)
        addr_trend = df['n_unique_addresses'].pct_change(24*7)
        df['Price_Addr_Div'] = addr_trend - price_trend
    
    return df


def calculate_kimchi_premium(df):
    """ê¹€ì¹˜ í”„ë¦¬ë¯¸ì—„ ê³„ì‚°"""
    df = df.copy()
    
    if 'close_upbit' in df.columns and 'exchange_rate' in df.columns and 'close' in df.columns:
        upbit_usd = df['close_upbit'] / df['exchange_rate']
        df['kimchi_premium'] = ((upbit_usd - df['close']) / df['close']) * 100
    else:
        df['kimchi_premium'] = 0
    
    return df

print("âœ… ê°€ê²©/ê±°ë˜ëŸ‰ íŒŒìƒ í”¼ì³ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ")

# ## ğŸ”¨ 11. í”¼ì³ ì—”ì§€ë‹ˆì–´ë§ - ê°ì„± ë°ì´í„° ë³‘í•© ë° ê¸°ë³¸ í”¼ì³
# 
# `raw_sentiment` í…Œì´ë¸”ì˜ ë°ì´í„°ë¥¼ ì‹œê°„ë³„ ë°ì´í„°ì— ë³‘í•©í•˜ê³  ê¸°ë³¸ ê°ì„± íŒŒìƒ í”¼ì³ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

def merge_sentiment_to_hourly(df, df_sentiment):
    """
    raw_sentiment ë°ì´í„°ë¥¼ ì‹œê°„ë³„ ë°ì´í„°ì— ë³‘í•©
    (ì¼ë³„ ê°ì„± â†’ í•´ë‹¹ ë‚ ì§œì˜ ëª¨ë“  ì‹œê°„ì— ê°™ì€ ê°’)
    """
    if df_sentiment.empty:
        df['sentiment_score'] = 0
        df['impact_score'] = 0
        return df
    
    # ì›ë³¸ ë³´í˜¸ë¥¼ ìœ„í•´ ë³µì‚¬
    df_sentiment = df_sentiment.copy()
    
    # ë‚ ì§œë§Œ ì¶”ì¶œí•˜ì—¬ ë³‘í•©
    df['date_only'] = df.index.date
    df_sentiment['date_only'] = df_sentiment.index.date
    
    # ì¤‘ë³µ ì œê±° (ê°™ì€ ë‚ ì§œ ì—¬ëŸ¬ ê°œ ìˆì„ ìˆ˜ ìˆìŒ)
    df_sentiment_daily = df_sentiment.groupby('date_only').first().reset_index()
    
    df = df.reset_index()
    df = df.merge(df_sentiment_daily[['date_only', 'sentiment_score', 'impact_score']], 
                  on='date_only', how='left', suffixes=('', '_new'))
    
    # ë³‘í•©ëœ ê°’ ì‚¬ìš© (ê¸°ì¡´ ê°’ì´ ì—†ê±°ë‚˜ 0ì¸ ê²½ìš°)
    if 'sentiment_score_new' in df.columns:
        df['sentiment_score'] = df['sentiment_score_new'].fillna(0)
        df['impact_score'] = df['impact_score_new'].fillna(0)
        df.drop(columns=['sentiment_score_new', 'impact_score_new'], inplace=True, errors='ignore')
    else:
        df['sentiment_score'] = df.get('sentiment_score', 0)
        df['impact_score'] = df.get('impact_score', 0)
    
    df.drop(columns=['date_only'], inplace=True)
    df.set_index('date', inplace=True)
    
    return df


def calculate_sentiment_features(df):
    """ê¸°ë³¸ ê°ì„± ì§€í‘œ íŒŒìƒ í”¼ì³ ê³„ì‚°"""
    df = df.copy()
    
    if 'sentiment_score' not in df.columns:
        df['sentiment_score'] = 0
        df['sentiment_missing_flag'] = 1
    else:
        df['sentiment_missing_flag'] = df['sentiment_score'].isna().astype(int)
        df['sentiment_score'] = df['sentiment_score'].fillna(0)
    
    if 'impact_score' not in df.columns:
        df['impact_score'] = 0
    
    df['sentiment_ma7'] = df['sentiment_score'].rolling(24*7, min_periods=1).mean()
    df['sent_ma_14'] = df['sentiment_score'].rolling(24*14, min_periods=1).mean()
    df['sent_diff'] = df['sentiment_score'].diff()
    df['sentiment_lag_1'] = df['sentiment_score'].shift(1)
    df['sent_volatility'] = df['sentiment_score'].rolling(24*7, min_periods=1).std()
    
    if 'net_liquidity' in df.columns:
        df['liq_x_sent'] = df['net_liquidity'] * df['sentiment_score']
    else:
        df['liq_x_sent'] = 0
    
    if 'fng_value' in df.columns:
        df['fng_fear_stress'] = (df['fng_value'] < 25).rolling(720, min_periods=1).mean()
        df['fng_greed_stress'] = (df['fng_value'] > 75).rolling(720, min_periods=1).mean()
    else:
        df['fng_value'] = 50
        df['fng_fear_stress'] = 0
        df['fng_greed_stress'] = 0
    
    if 'fng_fear_stress' in df.columns:
        df['winter_intensity'] = df['days_since_winter'] * df['fng_fear_stress'] if 'days_since_winter' in df.columns else 0
    
    return df

print("âœ… ê°ì„± ë³‘í•© ë° ê¸°ë³¸ í”¼ì³ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ")

# ## ğŸ”¨ 12. í”¼ì³ ì—”ì§€ë‹ˆì–´ë§ - ê³ ê¸‰ ê°ì„± í”¼ì³ (Part 1)
# 
# ê°ì„± ëª¨ë©˜í…€, ë ˆì§, ê°€ê²© ë‹¤ì´ë²„ì „ìŠ¤ ë“± ê³ ê¸‰ ê°ì„± í”¼ì³ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

def calculate_advanced_sentiment_features(df):
    """ê³ ê¸‰ ê°ì„± ì§€í‘œ ê³„ì‚°"""
    df = df.copy()
    
    if 'sentiment_score' not in df.columns:
        df['sentiment_score'] = 0
    sent = df['sentiment_score'].fillna(0)
    
    # Momentum
    df['sent_acceleration'] = sent.diff().diff(1)
    df['sent_momentum_3d'] = sent.diff(72)
    df['sent_momentum_7d'] = sent.diff(168)
    df['sent_momentum_14d'] = sent.diff(336)
    df['sent_velocity'] = sent.diff().rolling(24, min_periods=1).mean()
    
    # Regime
    try:
        df['sent_quartile'] = pd.qcut(sent, q=4, labels=[0, 1, 2, 3], duplicates='drop').astype(float)
    except:
        df['sent_quartile'] = 1.0
    
    sent_q10 = sent.quantile(0.1)
    sent_q90 = sent.quantile(0.9)
    df['sent_extreme_negative'] = (sent < sent_q10).astype(int)
    df['sent_extreme_positive'] = (sent > sent_q90).astype(int)
    
    try:
        df['sent_zone'] = pd.cut(sent, bins=[-float('inf'), -0.3, 0.3, float('inf')], labels=[-1, 0, 1]).astype(float)
    except:
        df['sent_zone'] = 0
    
    df['sent_regime_change'] = (df['sent_zone'] != df['sent_zone'].shift()).rolling(168, min_periods=1).sum()
    df['sent_neutral_days'] = (df['sent_zone'] == 0).rolling(168, min_periods=1).sum()
    
    # Price Divergence
    if 'close' in df.columns and 'log_ret_1' in df.columns:
        price_std = df['log_ret_1'].rolling(168, min_periods=1).std()
        price_norm = (df['log_ret_1'] / (price_std + 1e-8)).clip(-1, 1)
        df['sent_price_divergence'] = sent - price_norm
        df['sent_price_corr_7d'] = sent.rolling(168, min_periods=24).corr(df['close'])
        df['sent_price_corr_30d'] = sent.rolling(720, min_periods=168).corr(df['close'])
        df['divergence_extreme'] = (np.abs(df['sent_price_divergence']) > 1.5).astype(int)
        df['sent_price_alignment'] = ((sent > 0) == (df['log_ret_1'] > 0)).astype(int)
    else:
        for col in ['sent_price_divergence', 'sent_price_corr_7d', 'sent_price_corr_30d', 'divergence_extreme', 'sent_price_alignment']:
            df[col] = 0
    
    # Volatility
    df['sent_volatility_3d'] = sent.rolling(72, min_periods=24).std()
    df['sent_volatility_14d'] = sent.rolling(336, min_periods=72).std()
    df['sent_volatility_30d'] = sent.rolling(720, min_periods=168).std()
    df['sent_vol_change'] = df['sent_volatility'].diff(24) if 'sent_volatility' in df.columns else 0
    df['sent_vol_ratio'] = df['sent_volatility'] / (df['sent_volatility_30d'] + 1e-8) if 'sent_volatility' in df.columns else 0
    
    # Volume Interaction
    if 'volume' in df.columns:
        vol_mean = df['volume'].rolling(168, min_periods=24).mean()
        vol_std = df['volume'].rolling(168, min_periods=24).std()
        df['volume_zscore'] = (df['volume'] - vol_mean) / (vol_std + 1e-8)
        df['sent_vol_interaction'] = sent * df['volume_zscore']
        df['sent_vol_alignment'] = (((sent > 0.5) & (df['volume_zscore'] > 1)) | ((sent < -0.5) & (df['volume_zscore'] > 1))).astype(int)
        impact = df['impact_score'].fillna(0) if 'impact_score' in df.columns else 0
        df['high_impact_volume'] = ((impact > 0.7) & (df['volume_zscore'] > 1)).astype(int) if 'impact_score' in df.columns else 0
    else:
        for col in ['volume_zscore', 'sent_vol_interaction', 'sent_vol_alignment', 'high_impact_volume']:
            df[col] = 0
    
    return df

print("âœ… ê³ ê¸‰ ê°ì„± í”¼ì³ í•¨ìˆ˜ (Part 1) ì •ì˜ ì™„ë£Œ")

# ## ğŸ”¨ 13. í”¼ì³ ì—”ì§€ë‹ˆì–´ë§ - ê³ ê¸‰ ê°ì„± í”¼ì³ (Part 2)
# 
# ê°ì„± ì§€ì†ì„±, EMA, RSI, ë³¼ë¦°ì € ë°´ë“œ, MACD ë“±ì„ ê³„ì‚°í•©ë‹ˆë‹¤.

def calculate_advanced_sentiment_features_part2(df):
    """ê³ ê¸‰ ê°ì„± í”¼ì³ Part 2 - Persistence, EMA, RSI, BB, MACD"""
    df = df.copy()
    sent = df['sentiment_score'].fillna(0) if 'sentiment_score' in df.columns else pd.Series(0, index=df.index)
    
    # Persistence
    pos_mask = sent > 0
    pos_groups = (pos_mask != pos_mask.shift()).cumsum()
    df['sent_positive_streak'] = pos_mask.groupby(pos_groups).cumsum()
    
    neg_mask = sent < 0
    neg_groups = (neg_mask != neg_mask.shift()).cumsum()
    df['sent_negative_streak'] = neg_mask.groupby(neg_groups).cumsum()
    
    sent_dir = np.sign(sent)
    df['sent_direction_changes'] = (sent_dir != sent_dir.shift()).astype(int).rolling(168, min_periods=24).sum()
    df['sent_stability_7d'] = 1 / (sent.rolling(168, min_periods=24).std() + 1e-8)
    df['sent_reversal_signal'] = ((df['sent_momentum_3d'] > 0) & (df['sent_momentum_3d'].shift(1) < 0)).astype(int) if 'sent_momentum_3d' in df.columns else 0
    
    # EMA
    df['sent_ema_7'] = sent.ewm(span=7*24).mean()
    df['sent_ema_14'] = sent.ewm(span=14*24).mean()
    df['sent_ema_30'] = sent.ewm(span=30*24).mean()
    
    # RSI
    delta = sent.diff()
    gain = delta.where(delta > 0, 0).rolling(14*24).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(14*24).mean()
    rs = gain / (loss + 1e-9)
    df['sent_rsi'] = 100 - (100 / (1 + rs))
    df['sent_rsi_14'] = df['sent_rsi']
    
    # Bollinger Band
    sent_ma20 = sent.rolling(20*24).mean()
    sent_std20 = sent.rolling(20*24).std()
    df['sent_bb_upper'] = sent_ma20 + 2 * sent_std20
    df['sent_bb_lower'] = sent_ma20 - 2 * sent_std20
    df['sent_bb_width'] = (df['sent_bb_upper'] - df['sent_bb_lower']) / (sent_ma20 + 1e-9)
    df['sent_bb_position'] = (sent - df['sent_bb_lower']) / (df['sent_bb_upper'] - df['sent_bb_lower'] + 1e-9)
    
    # MACD
    ema12 = sent.ewm(span=12*24).mean()
    ema26 = sent.ewm(span=26*24).mean()
    df['sent_macd'] = ema12 - ema26
    df['sent_macd_signal'] = df['sent_macd'].ewm(span=9*24).mean()
    df['sent_macd_hist'] = df['sent_macd'] - df['sent_macd_signal']
    
    # Std/Skew
    df['sent_std_7'] = sent.rolling(7*24).std()
    df['sent_std_14'] = sent.rolling(14*24).std()
    df['sent_std_30'] = sent.rolling(30*24).std()
    df['sent_skew'] = sent.rolling(30*24).skew()
    df['sent_kurtosis'] = sent.rolling(30*24).kurt()
    
    # Misc
    if 'volume' in df.columns:
        df['sent_volume_corr_7'] = sent.rolling(7*24).corr(df['volume'])
    else:
        df['sent_volume_corr_7'] = 0
    
    if 'fng_value' in df.columns:
        df['sent_fng_diff'] = sent - df['fng_value'].fillna(50) / 100
    else:
        df['sent_fng_diff'] = 0
    
    df['sent_z_score'] = (sent - sent.rolling(30*24).mean()) / (sent.rolling(30*24).std() + 1e-9)
    df['sent_percentile'] = sent.rolling(365*24, min_periods=24).rank(pct=True)
    
    impact = df['impact_score'].fillna(0) if 'impact_score' in df.columns else pd.Series(0, index=df.index)
    df['impact_ma7'] = impact.rolling(7*24).mean()
    df['impact_volatility'] = impact.rolling(7*24).std()
    df['sent_impact_ratio'] = sent / (impact + 1e-9)
    
    return df

print("âœ… ê³ ê¸‰ ê°ì„± í”¼ì³ í•¨ìˆ˜ (Part 2) ì •ì˜ ì™„ë£Œ")

# ## ğŸ”§ 14. í†µí•© í”¼ì³ ê³„ì‚° ë° ê°ì„± ë³‘í•©
# 
# ëª¨ë“  í”¼ì³ ê³„ì‚° í•¨ìˆ˜ë“¤ì„ í†µí•© ì‹¤í–‰í•˜ê³  ê°ì„± ë°ì´í„°ë¥¼ ë³‘í•©í•©ë‹ˆë‹¤.

def calculate_all_features(df):
    """
    [Step 2] ëª¨ë“  í”¼ì³ ê³„ì‚°
    """
    logger.info("=" * 80)
    logger.info("Step 2: í”¼ì³ ì—”ì§€ë‹ˆì–´ë§")
    logger.info("=" * 80)
    
    df = calculate_technical_indicators(df)
    logger.info("  ê¸°ìˆ ì  ì§€í‘œ ì™„ë£Œ")
    
    df = calculate_onchain_proxies(df)
    logger.info("  ì˜¨ì²´ì¸ í”„ë¡ì‹œ ì™„ë£Œ")
    
    df = calculate_cycle_features(df)
    logger.info("  ì‚¬ì´í´/ì‹œê°„ í”¼ì³ ì™„ë£Œ")
    
    df = calculate_macro_derived_features(df)
    logger.info("  ë§¤í¬ë¡œ íŒŒìƒ í”¼ì³ ì™„ë£Œ")
    
    df = calculate_price_derived_features(df)
    logger.info("  ê°€ê²©/ê±°ë˜ëŸ‰ íŒŒìƒ í”¼ì³ ì™„ë£Œ")
    
    df = calculate_kimchi_premium(df)
    logger.info("  ê¹€ì¹˜ í”„ë¦¬ë¯¸ì—„ ì™„ë£Œ")
    
    return df


def merge_and_calculate_sentiment(df):
    """
    [Step 3] ê°ì„± ë°ì´í„° ë³‘í•© + ê°ì„± íŒŒìƒ í”¼ì³ ê³„ì‚°
    """
    logger.info("=" * 80)
    logger.info("Step 3: ê°ì„± ë°ì´í„° ë³‘í•© ë° íŒŒìƒ í”¼ì³ ê³„ì‚°")
    logger.info("=" * 80)
    
    # raw_sentiment ë¡œë“œ
    df_sentiment = fetch_all_raw_sentiment()
    
    # ì‹œê°„ë³„ ë°ì´í„°ì— ë³‘í•©
    df = merge_sentiment_to_hourly(df, df_sentiment)
    logger.info(f"  ê°ì„± ë°ì´í„° ë³‘í•© ì™„ë£Œ")
    
    # ê°ì„± íŒŒìƒ í”¼ì³ ê³„ì‚°
    df = calculate_sentiment_features(df)
    logger.info("  ê¸°ë³¸ ê°ì„± í”¼ì³ ì™„ë£Œ")
    
    df = calculate_advanced_sentiment_features(df)
    logger.info("  ê³ ê¸‰ ê°ì„± í”¼ì³ (Part 1) ì™„ë£Œ")
    
    df = calculate_advanced_sentiment_features_part2(df)
    logger.info("  ê³ ê¸‰ ê°ì„± í”¼ì³ (Part 2) ì™„ë£Œ")
    
    return df

print("âœ… í†µí•© í”¼ì³ ê³„ì‚° í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ")

# ## ğŸ’¾ 15. ë°ì´í„° ì •ì œ ë° DB ì €ì¥
# 
# NaN/Inf ì²˜ë¦¬, ì¤‘ë³µ ì œê±°, DB ìŠ¤í‚¤ë§ˆ ê²€ì¦ í›„ `features_master` í…Œì´ë¸”ì— ì €ì¥í•©ë‹ˆë‹¤.

def clean_data(df):
    """
    [Step 4] ë°ì´í„° ì •ì œ
    """
    logger.info("=" * 80)
    logger.info("Step 4: ë°ì´í„° ì •ì œ")
    logger.info("=" * 80)
    
    # NaN/Inf ì²˜ë¦¬
    df = df.replace([np.inf, -np.inf], 0).fillna(0)
    
    # ì¤‘ë³µ ì œê±°
    df = df[~df.index.duplicated(keep='first')]
    
    # ì •ë ¬
    df = df.sort_index()
    
    logger.info(f"  ì •ì œ ì™„ë£Œ: {len(df)} rows, {len(df.columns)} columns")
    return df


def get_features_master_schema():
    """features_master í…Œì´ë¸”ì˜ ì»¬ëŸ¼ ëª©ë¡ ì¡°íšŒ (í…Œì´ë¸”ì´ ë¹„ì–´ìˆì–´ë„ ì‘ë™)"""
    try:
        # ë°©ë²• 1: ê¸°ì¡´ ë°ì´í„°ì—ì„œ ì»¬ëŸ¼ ì¶”ì¶œ
        response = supabase.table("features_master").select("*").limit(1).execute()
        if response.data:
            return list(response.data[0].keys())
        
        # ë°©ë²• 2: í…Œì´ë¸”ì´ ë¹„ì–´ìˆìœ¼ë©´ RPCë¡œ ìŠ¤í‚¤ë§ˆ ì¡°íšŒ ì‹œë„
        try:
            rpc_response = supabase.rpc("get_table_columns", {"table_name": "features_master"}).execute()
            if rpc_response.data:
                return [col['column_name'] for col in rpc_response.data]
        except:
            pass
        
        # ë°©ë²• 3: model_features.jsonì—ì„œ full_dataset ë¡œë“œ
        logger.warning("DB ìŠ¤í‚¤ë§ˆ ì¡°íšŒ ì‹¤íŒ¨, model_features.jsonì—ì„œ ë¡œë“œ")
        try:
            json_path = '/content/drive/MyDrive/2526Winter_Sideproject/model_features.json'
            with open(json_path, 'r', encoding='utf-8') as f:
                features_config = json.load(f)
            if 'full_dataset' in features_config:
                logger.info(f"  model_features.jsonì—ì„œ {len(features_config['full_dataset'])}ê°œ ì»¬ëŸ¼ ë¡œë“œ")
                return features_config['full_dataset']
        except Exception as json_err:
            logger.warning(f"JSON ë¡œë“œ ì‹¤íŒ¨: {json_err}")
        
        # ë°©ë²• 4: ìµœì†Œ í•„ìˆ˜ ì»¬ëŸ¼ (ìµœí›„ì˜ ìˆ˜ë‹¨)
        logger.warning("ìµœì†Œ í•„ìˆ˜ ì»¬ëŸ¼ë§Œ ì‚¬ìš©")
        return ['date', 'open', 'high', 'low', 'close', 'volume']
        
    except Exception as e:
        logger.error(f"ìŠ¤í‚¤ë§ˆ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        return []


def save_to_database(df):
    """
    [Step 5] features_masterì— ì €ì¥
    """
    logger.info("=" * 80)
    logger.info("Step 5: DB ì €ì¥")
    logger.info("=" * 80)
    
    # ë‚ ì§œ í˜•ì‹ ë³€í™˜: "2026-02-05 16:00:00"
    df['date'] = df.index.strftime('%Y-%m-%d %H:%M:%S')
    
    # DB ìŠ¤í‚¤ë§ˆ í™•ì¸í•˜ì—¬ ìœ íš¨í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ
    valid_columns = get_features_master_schema()
    
    if valid_columns:
        # DBì— ì—†ëŠ” ì»¬ëŸ¼ í™•ì¸ ë° ë¡œê¹…
        missing_in_db = [c for c in df.columns if c not in valid_columns and c != 'date']
        if missing_in_db:
            logger.warning(f"  DB ìŠ¤í‚¤ë§ˆì— ì—†ëŠ” ì»¬ëŸ¼ {len(missing_in_db)}ê°œ ì œì™¸:")
            for col in missing_in_db[:10]:
                logger.warning(f"    - {col}")
            if len(missing_in_db) > 10:
                logger.warning(f"    ... ì™¸ {len(missing_in_db) - 10}ê°œ")
        
        # ìœ íš¨í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ (date ì»¬ëŸ¼ì€ í•­ìƒ í¬í•¨)
        use_columns = [c for c in df.columns if c in valid_columns or c == 'date']
        df = df[use_columns]
        logger.info(f"  ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì™„ë£Œ: {len(use_columns)}ê°œ ì»¬ëŸ¼ ì‚¬ìš©")
    else:
        logger.warning("  DB ìŠ¤í‚¤ë§ˆë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŒ! ëª¨ë“  ì»¬ëŸ¼ ì‹œë„...")
    
    # INT4 íƒ€ì… ì»¬ëŸ¼ ëª©ë¡ (DB ìŠ¤í‚¤ë§ˆì™€ ì¼ì¹˜í•´ì•¼ í•¨)
    INT_COLUMNS = {
        'fng_value', 'golden_cross', 'Vol_Anomaly', 
        'days_since_halving', 'halving_cycle_index', 'days_since_winter',
        'hour', 'dayofweek', 'is_weekend', 
        'is_ny_open', 'is_london_close', 'session_overlap', 'is_us_market_open',
        'Real_Yield_Impact', 'vix_spike', 
        'sentiment_missing_flag', 'sent_extreme_negative', 'sent_extreme_positive',
        'divergence_extreme', 'sent_price_alignment', 'sent_vol_alignment',
        'high_impact_volume', 'sent_reversal_signal', 'label_binary'
    }
    
    # ë ˆì½”ë“œë¡œ ë³€í™˜
    records = df.to_dict(orient='records')
    
    # ìˆ«ìí˜• ë°ì´í„° ì •ì œ - INT vs FLOAT êµ¬ë¶„
    for r in records:
        for key, value in r.items():
            if pd.isna(value):
                r[key] = 0
            elif key in INT_COLUMNS:
                # INT4 ì»¬ëŸ¼ì€ ì •ìˆ˜ë¡œ ë³€í™˜
                try:
                    r[key] = int(float(value))
                except (ValueError, TypeError):
                    r[key] = 0
            elif isinstance(value, (np.floating, np.integer)):
                r[key] = float(value)
    
    logger.info(f"  ì €ì¥í•  ë ˆì½”ë“œ: {len(records)}ê°œ")
    
    # ë°°ì¹˜ upsert
    batch_size = 500
    error_count = 0
    success_count = 0
    last_error = None
    
    for i in tqdm(range(0, len(records), batch_size), desc="DB ì €ì¥"):
        batch = records[i:i+batch_size]
        try:
            supabase.table("features_master").upsert(batch, on_conflict="date").execute()
            success_count += len(batch)
        except Exception as e:
            error_count += 1
            last_error = str(e)
            logger.error(f"ë°°ì¹˜ {i//batch_size + 1} ì €ì¥ ì‹¤íŒ¨: {e}")
    
    # ìµœì¢… ê²°ê³¼ ë¡œê¹…
    logger.info("=" * 40)
    logger.info(f"DB ì €ì¥ ê²°ê³¼:")
    logger.info(f"  - ì„±ê³µ: {success_count}ê°œ ë ˆì½”ë“œ")
    logger.info(f"  - ì‹¤íŒ¨ ë°°ì¹˜: {error_count}ê°œ")
    if last_error:
        logger.error(f"  - ë§ˆì§€ë§‰ ì—ëŸ¬: {last_error[:200]}")
    logger.info("=" * 40)


def sync_market_realtime(df):
    """
    features_master ìµœì‹  1í–‰ì„ market_realtime í˜•ì‹ìœ¼ë¡œ INSERT
    (INSERTë§Œ ì‚¬ìš© - PK/on_conflict ì—†ì´ ì‹ ê·œ í–‰ ì¶”ê°€)
    """
    if df.empty:
        return
    row = df.iloc[-1]  # ìµœì‹  í–‰
    data = {
        'timestamp': df.index[-1].strftime('%Y-%m-%d %H:%M:%S+00:00'),
        'usd_krw_rate': float(row.get('exchange_rate', 0)),
        'btc_usd_price': float(row.get('close', 0)),
        'btc_krw_price': float(row.get('close_upbit', 0)),
        'kimchi_premium': float(row.get('kimchi_premium', 0)),
    }
    try:
        supabase.table('market_realtime').insert(data).execute()
        logger.info("market_realtime ë™ê¸°í™” ì™„ë£Œ (INSERT)")
    except Exception as e:
        logger.warning(f"market_realtime ë™ê¸°í™” ì‹¤íŒ¨: {e}")

print("âœ… ë°ì´í„° ì •ì œ ë° DB ì €ì¥ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ")

# ## ğŸš€ 16. ë©”ì¸ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
# 
# ëª¨ë“  ë‹¨ê³„ë¥¼ í†µí•©í•˜ì—¬ ì‹¤í–‰í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜ì…ë‹ˆë‹¤.
# 
# **ëª¨ë“œ ì„ íƒ:**
# - `INCREMENTAL`: DBì˜ ë§ˆì§€ë§‰ ë‚ ì§œ ì´í›„ë§Œ ìˆ˜ì§‘ (ê¸°ë³¸ê°’, ì¶”ì²œ)
# - `FULL`: 2017-01-01ë¶€í„° ì „ì²´ ìˆ˜ì§‘ (ì£¼ì˜: ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼)

def main(mode="INCREMENTAL"):
    """
    ë©”ì¸ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    
    Args:
        mode: "FULL" - 2017-01-01ë¶€í„° ì „ì²´ ìˆ˜ì§‘
              "INCREMENTAL" - DBì˜ ë§ˆì§€ë§‰ ë‚ ì§œ ì´í›„ë§Œ ìˆ˜ì§‘ (ê¸°ë³¸ê°’)
    """
    logger.info("=" * 80)
    logger.info(f"29_COMPLETE_PIPELINE ì‹œì‘ - ëª¨ë“œ: {mode}")
    logger.info("=" * 80)
    
    # INCREMENTAL ëª¨ë“œì—ì„œ í”¼ì³ ê³„ì‚°ìš© lookback ê¸°ê°„ (ì¼ ë‹¨ìœ„)
    # HV_Rank ë“± ìµœëŒ€ 365ì¼ rolling window ëŒ€ì‘ + ì—¬ìœ ë¶„
    LOOKBACK_DAYS = 400
    
    try:
        # ì‹œì‘/ì¢…ë£Œ ë‚ ì§œ ê²°ì •
        if mode == "FULL":
            start_date = datetime(2017, 1, 1, tzinfo=timezone.utc)
            crawl_start_date = start_date
            save_start_date = start_date
            logger.info("FULL ëª¨ë“œ: 2017-01-01ë¶€í„° ì „ì²´ ìˆ˜ì§‘")
        else:
            last_date = get_latest_date_from_db()
            if last_date:
                save_start_date = last_date + timedelta(hours=1)
                # í”¼ì³ ê³„ì‚°ì„ ìœ„í•´ lookback ê¸°ê°„ë§Œí¼ ë” ì´ì „ë¶€í„° ë°ì´í„° ìˆ˜ì§‘
                crawl_start_date = save_start_date - timedelta(days=LOOKBACK_DAYS)
                start_date = save_start_date  # ê°ì„± ë¶„ì„ì€ ì‹ ê·œ ë‚ ì§œë¶€í„°ë§Œ
                logger.info(f"INCREMENTAL ëª¨ë“œ: {last_date}ë¶€í„° ìˆ˜ì§‘")
                logger.info(f"  í”¼ì³ lookback: {crawl_start_date.strftime('%Y-%m-%d')}ë¶€í„° í¬ë¡¤ë§")
            else:
                start_date = datetime(2017, 1, 1, tzinfo=timezone.utc)
                crawl_start_date = start_date
                save_start_date = start_date
                logger.info("DBê°€ ë¹„ì–´ìˆìŒ. 2017-01-01ë¶€í„° ì‹œì‘")
        
        end_date = datetime.now(timezone.utc)
        
        # ì´ë¯¸ ìµœì‹ ì¸ ê²½ìš° ì¢…ë£Œ
        if save_start_date >= end_date:
            logger.info("ì´ë¯¸ ìµœì‹  ìƒíƒœì…ë‹ˆë‹¤. ìˆ˜ì§‘í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        logger.info(f"ìˆ˜ì§‘ ê¸°ê°„: {start_date} ~ {end_date}")
        
        # Step 1A: ë‰´ìŠ¤ ìˆ˜ì§‘ + GPT ê°ì„± ë¶„ì„ â†’ raw_sentiment ì €ì¥
        collect_and_save_sentiment(start_date, end_date)
        
        # Step 1B: ê°€ê²©/ì§€í‘œ ë°ì´í„° ìˆ˜ì§‘ (lookback í¬í•¨)
        df = collect_all_data(crawl_start_date, end_date)
        if df.empty:
            logger.error("ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨!")
            return
        
        # Step 2: í”¼ì³ ì—”ì§€ë‹ˆì–´ë§
        df = calculate_all_features(df)
        
        # Step 3: ê°ì„± ë°ì´í„° ë³‘í•© + ê°ì„± íŒŒìƒ í”¼ì³
        df = merge_and_calculate_sentiment(df)
        
        # Step 4: ë°ì´í„° ì •ì œ
        df = clean_data(df)
        
        # INCREMENTAL ëª¨ë“œ: lookback ë°ì´í„° ì œì™¸, ì‹ ê·œ ë°ì´í„°ë§Œ ì €ì¥
        if mode == "INCREMENTAL" and crawl_start_date != save_start_date:
            save_start_naive = save_start_date.replace(tzinfo=None)
            original_len = len(df)
            df = df[df.index >= save_start_naive]
            logger.info(f"  INCREMENTAL íŠ¸ë¦¬ë°: {original_len} â†’ {len(df)}ê°œ (ì‹ ê·œ ë°ì´í„°ë§Œ ì €ì¥)")
        
        if df.empty:
            logger.info("ì €ì¥í•  ì‹ ê·œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # Step 5: DB ì €ì¥
        save_to_database(df)
        sync_market_realtime(df)
        
        logger.info("=" * 80)
        logger.info("ëª¨ë“  íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
        logger.info("=" * 80)
        
    except Exception as e:
        logger.error(f"íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        raise e

print("âœ… ë©”ì¸ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ")

# ## â–¶ï¸ 17. íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
# 
# ì•„ë˜ ì…€ì„ ì‹¤í–‰í•˜ì—¬ íŒŒì´í”„ë¼ì¸ì„ ì‹œì‘í•©ë‹ˆë‹¤.
# 
# **ì°¸ê³ :**
# - ì²« ì‹¤í–‰ì´ê±°ë‚˜ ì „ì²´ ìˆ˜ì§‘ì´ í•„ìš”í•œ ê²½ìš°: `main(mode="FULL")`
# - ì¼ìƒì ì¸ ì—…ë°ì´íŠ¸: `main(mode="INCREMENTAL")` (ê¸°ë³¸ê°’)

# íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (INCREMENTAL ëª¨ë“œ - ìµœì‹  ë°ì´í„°ë§Œ ì¶”ê°€)
main(mode="INCREMENTAL")

# ì „ì²´ ìˆ˜ì§‘ì´ í•„ìš”í•œ ê²½ìš° ì•„ë˜ ì£¼ì„ í•´ì œ:
# main(mode="FULL")
